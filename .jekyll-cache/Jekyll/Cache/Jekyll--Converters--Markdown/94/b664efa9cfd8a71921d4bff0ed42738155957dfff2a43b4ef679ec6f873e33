I"bh<p><small>최종 수정일 : 2019-10-13</small></p>

<p>우리는 <em>Neural Network</em> 를 학습시키기 위한 기초적인 것들을 살펴보았다.<br />
이제 <em>Neural Network</em> 를 학습시키는 과정과 그 속에서 필요한 것들에 대해 알아보자.</p>

<h2 id="overview">Overview</h2>

<p><em>NN</em> 을 만드는 과정은 다음과 같이 정리할 수 있다.</p>

<ol>
  <li>초기 설정 : 활성 함수(<em>activation function</em>), 전처리(<em>preprocessing</em>), 가중치 초기화(<em>weight initialization</em>), 정규화(<em>regularization</em>), <em>gradient checking</em>(<em>sanity check</em>)</li>
  <li>학습 : <em>learning process</em>, <em>parameter update</em>, <strong>hyperparameter</strong> optimization</li>
  <li>평가 : 앙상블(<em>model ensemble</em>), <em>test-time augmentation</em>(테스트 시 데이터셋을 늘리는 기법)</li>
</ol>

<p><a name="activation-functions"></a></p>

<p>순서대로 한 번 살펴보자.</p>

<h2 id="activation-functions">Activation Functions</h2>

<p>우리는 이미 <em>perceptron</em> 모델과 여러 활성 함수들에 대해 간략히 살펴 보았다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image1.png" alt="perceptron and activation functions" /></p>

<p>각 활성 함수들에 대해 조금 더 자세히 알아보자.</p>

<h3 id="sigmoid">Sigmoid</h3>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image2.png" alt="sigmoid" /></p>

<p>0~1 사이의 값을 갖도록 하는 sigmoid 는 뉴런의 ‘firing rate’ 의 동작 구조와 상당히 비슷해서 많이 쓰였었다.<br />
그러나 현재에는 거의 쓰이지 않는 함수인데 다음과 같은 세 가지 문제점을 볼 수 있다.</p>

<ol>
  <li><strong><em>kill gradient</em></strong><br />
그래프를 보면 알 수 있듯이 양 극단에서의 기울기가 0에 수렴하는 현상이 있다.<br />
saturated regime 이라고 하는 이 구간에 대해서는 기울기가 0에 가까워 <em>gradient update</em> 가 사실상 일어나지 않게 된다.<br />
이렇게 <em>saturated neuron</em> 이 생기게 되고 이들은 gradient 를 <strong><em>kill</em></strong> 하게 된다.<br />
즉, x=-10 또는 x=10 과 같은 경우에 <em>backpropagation</em> 이 <strong><em>stopped</em></strong> 하게 되는 문제가 발생한다.</li>
  <li><strong><em>non zero-centered</em></strong><br />
그래프를 보면 <em>sigmoid</em> 함수는 그 중간이 0(원점)이 아님을 알 수 있는데, 함수의 모양에서도 볼 수 있듯이 어떤 x 에 대해서도 항상 양수(0보다 항상 큼)의 출력 값을 가진다는 것을 볼 수 있다.<br />
<em>chain rule</em> 에 의해 우리는 다음과 같은 식 ${dL \over dw_i} = {dL \over df} \times {df \over dw_i} = {dL \over df} \times x_i$(여기서, $f\left( \sum_iw_ix_i + b \right)$)을 구할 수 있고, $x_i$ 가 양수면 <em>gradient</em> 도 양수, $x_i$ 가 음수면 <em>gradient</em> 도 모두 음수가 된다. 즉,<br />
<img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image3.png" alt="sigmoid gradient" /><br />
그림에서 볼 수 있듯이 <em>hypothetical optimal w vector</em> 가 파란색처럼인 경우 <em>inefficient update</em> 를 하게 된다. 이는 <em>gradient</em> 가 모두 같은 부호의 값을 가지기 때문이고, 이는 zero mean 이 아니기 때문이다.</li>
  <li><strong><em>exp 연산</em></strong><br />
exp 연산은 계산 상 그 cost 가 높은 부분이다. 이는 조금 minor 이슈인데, 우리는 이미 deep learning 에서 <em>dot product</em> 등의 연산을 하게 되기 때문이다.</li>
</ol>

<h3 id="tanhhyperbolic-tangent">tanh(hyperbolic tangent)</h3>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image4.png" alt="tanh" /></p>

<p>이는 zero mean 을 가지게 되어 <em>sigmoid</em> 에서 갖고 있던 <em>zero-centered</em> 문제를 해결할 수 있으나 여전히 saturated 의 경우 gradient 를 kill 한다.</p>

<h3 id="reluretified-linear-unit">ReLU(Retified Linear Unit)</h3>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image5.png" alt="relu" /></p>

<p>가장 보편적으로 쓰이는 활성 함수이다.<br />
우리가 앞서 보았던 $f(x) = max(0,x)$인데 다음과 같은 이점이 있다.</p>

<ul>
  <li>+ 영역에서 saturate 하지 않는다.</li>
  <li>linear 이기에 계산이 매우 효율적이다.</li>
  <li><em>sigmoid, tanh</em> 보다 매우 빠르게 수렴함을 보인다.(약 6배)</li>
</ul>

<p>그러나 zero-centered 하지 않은 output 을 내보내는 문제점이 있다.<br />
이는 half regime(- 영역)에서 <em>kill gradient</em> 하게 되는 문제점도 있으며, 0에서는 미분불가능하기 때문에 <em>gradient</em> 가 undefined 하게 된다.</p>

<p>또한 간헐적으로 <em>dead ReLU</em> 가 되기도 하는데, weight 를 잘못 초기화 하거나 learning rate 가 너무 높을 때 발생하게 된다. <em>dead ReLU</em> 는 입력 값에 대해서 하나도 활성화되지 않는 경우를 뜻하는데, 이차원 평면에서의 예제를 보면 아래와 같은 경우이다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image8.png" alt="dead relu" /></p>

<p>현실에서는 이차원 평면이 아닌 다차원 공간에서의 hyperplane 으로 나뉘는 공간에 대해 activation 하는 점을 명심하자.<br />
이러한 dead ReLU 현상을 피하기 위해 사람들은 positive bias(0.01) 정도를 초기에 설정하게 되는데 이는 검증되지는 않았지만 그냥 그러한 경향으로 사용하고 있다.</p>

<h3 id="leacky-relu">Leacky ReLU</h3>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image6.png" alt="leacky relu" /></p>

<p><em>ReLU</em> 에서 조금 변형한 함수로, <em>kill gradient</em> 가 일어나지 않도록 - 영역에서 slight slope(0.01)의 값을 이용해 no saturating regime 을 만들어냈다.</p>

<p>그러나 이는 앞서 말했듯이 검증되지 않은 활성 함수이다.</p>

<p>덧붙여서, 저 slight slope 도 파라미터로 취급해 학습하면 더 낫지 않을까라는 생각에 <em>PReLU(Parametric Rectifier)</em> 라는 함수를 쓰기도 한다.<br />
$f(x) = max(\alpha x, x)$<br />
이는 조금 더 activation 에 유연함(flexibility)을 부여하게 해준다.</p>

<h3 id="elu">ELU</h3>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image7.png" alt="elu" /></p>

<p><em>ReLU</em> 의 이점을 모두 가지며 zero mean 에 가까운 output 을 내도록 하는 활성 함수이다. <em>Leacky ReLU</em> 와는 다르게 negative saturation regime(- 영역이 깊어질 수록)이 존재한데, 사실 이것은 noise 에 좀 더 강한 robustness 를 부여하게 된다.</p>

<blockquote>
  <p>negative 는 noise 일 확률이 높기 때문에(그 값이 true negative 라면) 학습을 하면 overfitting 되거나 성능에 악영향을 미칠 수 있다.</p>
</blockquote>

<p>$f(x) = \begin{cases} x &amp; \text{if } x \gt 0 \ \alpha \left( e^x -1 \right) &amp; \text{if } x \le 0 \end{cases}$</p>

<p>위의 negative 영역에서의 함수는 의도적으로 설계된 함수인데, 관련 논문을 읽어보자!</p>

<p>이는 꽤 괜찮은 함수이나 exp 연산으로 인해 computational cost 가 증가한다는 단점이 있다.</p>

<h3 id="maxout">Maxout</h3>

<p><em>ReLU</em> 와 <em>Leacky ReLU</em> 를 일반화한 것으로 linear regime 을 가지고 saturate 하지 않고 kill gradient(die)도 없다.</p>

<p>$max \left( w_1^Tx + b_1, w_2^Tx + b_2 \right)$</p>

<p>로 표현되나 파라미터/뉴런의 개수가 2배가 된다는 문제가 있다.</p>

<hr />

<p>여기까지 정리하면, 기본적으로 <strong>ReLU</strong> 를 사용하는 것이 좋다.(보편적)<br />
learning rate 를 조심스럽게 설정하여 여러 시도를 통해 적합한 learning rate 를 찾아보고, 다른 활성 함수들도 적용해보면 좋다.</p>

<p>그러나 <em>tanh</em> 는 좋은 성능을 내기는 어렵고, <em>sigmoid</em> 는 <strong><em>사용하지 마라</em></strong>.</p>

<h2 id="data-preprocessing">Data Preprocessing</h2>

<p>데이터 전처리는 <em>Data Science</em> 와 <em>Machine Learning</em> 분야에 있어서 매우 중요한 과정이다.<br />
우리가 앞서 보았던 문제점 중 <em>zero-centered</em> 하지 않아서 발생하는 문제가 있었다.<br />
데이터를 중앙으로 모으면(<em>zero-centered</em>) 어느 정도 해결할 수 있을 것임을 유추할 수 있다. <small>어느 정도라고 하는 이유는 layer 가 깊어질 수록, 즉 forward pass 로 진행이 될 수록 문제점들이 계속 나타나고 이러한 과정만으로는 완벽하게 보정하기 힘들기 때문이다. 그래서 이런 preprocessing 과정은 가장 앞쪽 layer에서 보통 해준다.</small></p>

<p><em>data preprocessing</em> 과정에는 <em>normalization</em> 도 있는데, 이는 같은 영역 안에 데이터들이 위치하도록 만들어서 기여하는 영향의 정도를 동일하게 만들어주는 의미를 가진다. 그러나 이미지 영역에서는 이미 pixel 들이 localty 라던가 하는 지역적 정보를 가지고 있기에 이러한 normalization 은 좋지 못하다.</p>

<p>그 외에도 <em>PCA</em>, <em>whitening</em> 등도 image(pixel value)에는 적합하지 못한 전처리 과정이다.</p>

<p>그러므로 image 에는 <strong><em>center</em></strong> 만 전처리로 해주자!</p>

<p>실제 <em>ConvNet</em> 들이 해준 <em>center</em> 과정은 아래와 같다.</p>

<ul>
  <li>이미지의 평균 값을 빼줌(<em>AlexNet</em>)</li>
  <li>채널의 평균 값을 빼줌(<em>VGGNet</em>)</li>
  <li>채널의 평균 값을 빼고 분산으로 나눠줌(<em>ResNet</em>)<small>정규분포의 냄새가 나지 않는가?</small></li>
</ul>

<h2 id="weight-initialization">Weight Initialization</h2>

<p>가중치 초기화는 굉장히 중요한 부분이다. 다음의 <em>FC layer</em> 모형에 대해 잠깐 생각하는 시간을 가져보자.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image9.png" alt="fc layer" /></p>

<p>이 때, $W$ 의 값이 모두 같다면 어떤 현상이 있게 될까?</p>

<blockquote>
  <p>dead, 죽는다고 생각할 수도 있는데 죽는 것은 아니고 모든 neuron 에 같은 일이 일어나게 된다. 이는 모두 동일한 output 을 내보내게 되고, backpropagation 할 때도 모두 같은 gradient 를 갖게 된다. 이는 뉴런의 비대칭성(asymmetric)을 해치는 것으로도 볼 수 있다.</p>
</blockquote>

<p>이렇듯 단순하게 같은 값으로 초기화(0, 1, …)하면 안되니 어떻게 초기화를 하는 것이 좋을까?</p>

<h3 id="1-작은-임의의-수로-초기화">1. 작은 임의의 수로 초기화</h3>

<p>무작위의 숫자로 초기화(+ gaussian 분포)하면 어떤 결과를 얻게되는지 살펴보자.<br />
이 방법은 작은 network 에서는 괜찮으나 network 가 깊어지면 문제가 발생한다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image10.png" alt="small random weight init" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4096</span><span class="p">]</span> <span class="o">*</span> <span class="mi">7</span>
<span class="n">hs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">Din</span><span class="p">,</span> <span class="n">Dout</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">W</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Din</span><span class="p">,</span> <span class="n">Dout</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">))</span>
    <span class="n">hs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>위의 결과는 6-layer with hidden size 4096 의 net에서 forward pass 를 진행한 결과이다. <em>tanh</em> 를 활성 함수로 사용하였는데 이 함수는 zero-centered 이기에 평균이 0 이 나오는 것을 확인 할 수 있다.<br />
문제는 분산(<em>std</em>) 의 값이 빠르게 0으로 수렴하는 것인데, 이는 activation 이 0이 된다는 의미고 다시 말해 <em>update</em> 가 일어나지 않게 된다는 뜻이다. $\to$ <strong><em>die</em></strong></p>

<p>이렇게 <em>gradient = 0</em> 이 되어버리는 문제가 있는데, 작은 수로 초기화 했기 때문에 이런 현상이 일어나는 것은 아닐까?</p>

<p>이번에는</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Din</span><span class="p">.</span> <span class="n">Dout</span><span class="p">)</span>
</code></pre></div></div>

<p>으로 늘렸을 때를 살펴보자.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image11.png" alt="increased random weight init" /></p>

<p>여전히 <em>gradient = 0</em> 이 되는 것을 피할 수는 없다.</p>

<p>이런 식으로는 학습을 진행할 수 없는데, 2010년 <strong><em>Xavier</em></strong> <em>initialization</em> 이라는 방법이 <em>Glorot</em> 에 의해 고안되었다.<br />
이 방식은 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Din</span><span class="p">,</span> <span class="n">Dout</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Din</span><span class="p">)</span>
</code></pre></div></div>

<p>이렇게 초기화한 값을 통해서 다음과 같은 결과를 얻을 수 있는데</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image12.png" alt="xavier weight init" /></p>

<p>적당하게 수렴하는 모습을 볼 수 있으며, 성능 역시 적절하게 나타나는 것을 알 수 있었다.<br />
<small>activation 이 적절하게 모든 layer 에서 scaled 되는 것을 알 수 있음</small></p>

<blockquote>
  <p><em>conv. layer</em> 에서 <em>Din</em> 은 kerner size^2 * input_channels
Xavier Initialization 의 유도 과정</p>
</blockquote>

\[\begin{aligned}
Var(y_i)    &amp;= D_{in} \times Var(x_iw_i) \\
            &amp;= D_{in} \times \left( E\left[x_i^2 \right] E\left[ w_i^2 \right] - E \left[ x_i \right]^2 E\left[w_i \right]^2 \right)\\
            &amp;= D_{in} \times Var(x_i) \times Var(w_i)
\end{aligned}\]

<p><br /></p>

\[\text{if } Var(w_i) = {1 \over D_{in}} \quad \text{then } Var(y_i) = Var(x_i)\]

<p>이는 간단하게 정리를 해 본 것인데, 이렇게 전개가 가능한 것은 다음의 가정들이 있기에 가능하다.</p>

<ul>
  <li>$w_i, x_i$ 는 동일한 확률분포를 가짐</li>
  <li>두 개 모두 평균 0(<em>ReLU</em> 에서는 평균이 0보다 조금 크기에 항상 적용할 수 있는 것은 아닌 것에 주의!)</li>
</ul>

<p>마지막의 ${1 \over D_{in}}$ 이 되기 위해서 분산의 성질을 생각해보면 $\sqrt{D_{in}}$으로 가중치를 초기화해주면 된다는 것을 알 수 있다. <small>$Var(aX) = a^2Var(X)$</small></p>

<p>지금까지는 <em>tanh</em> 을 이용했는데 <em>ReLU</em> 에 <em>Xavier initialization</em> 을 적용하면 어떻게 될까?</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image13.png" alt="xavier init with relu" /></p>

<p>위에서 주의사항을 언급했듯이, <em>ReLU</em> 의 평균은 0보다 크나, <em>Xavier init</em> 은 <em>zero-centered</em> 를 가정한 원리이기에 문제가 발생한다.<br />
또다시 activation 이 0 으로 collapse 하게 되어 학습이 진행되지 않게 되는데, 2015년 He 에 의해 적당히 초기화하는 방법이 만들어졌다.</p>

<p>이는 <em>Xavier init</em> 개선시킨 것인데, 놀랍게도 아주 간단한 개선이지만 성능에는 많은 영향을 끼친다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">Din</span><span class="p">,</span> <span class="n">Dout</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">Din</span><span class="p">)</span>
</code></pre></div></div>

<p>차이점이 느껴지는가? <em>ReLU</em> 의 특성을 보정하기 위해 sqrt(Din) 대신 sqrt(2/Din) 이 사용되었다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image14.png" alt="kaiming init with relu" /></p>

<p>그 결과, activation 이 다시 적당하게 되게 되었다.</p>

<p><em>Xavier/2</em> 로도 불리는 이 기법이 현재에는 가장 잘 쓰이는 초기화 기법이다.</p>

<h2 id="batch-normalization">Batch Normalization</h2>

<p>우리는 데이터가 gaussian range에서 activation 이 꾸준히 잘 되기를 원하고 있다.</p>

<p>그렇다면 그렇게 하면 되지 않는가?<br />
에서 출발한 것이 바로 배치 정규화(<em>Batch Normalization</em>)이다.</p>

<p>현재 배치에서 정규분포화 하기 위해 다음을 적용하는 것이다.</p>

\[\hat{x}^{(k)} = {x^{(k)} - E\left[ x^{\left( k \right)} \right] \over \sqrt{Var\left[ x^{(k)} \right]}}\]

<p>이러한 배치 정규화는 training 때에만 진행해야 한다.<br />
test 시에는 학습된 값을 이용해 구하기만 한다.</p>

\[\begin{aligned}
\mu_\beta &amp;\leftarrow {1 \over m}\sum_{i=1}^m x_i \qquad &amp;\text{ mini-batch 평균}\\
\sigma_\beta^2 &amp;\leftarrow {1\over m}\sum_{i=1}^m\left(x_i - \mu_\beta \right)^2 \qquad &amp;\text{ mini-batch 분산}\\
\hat{x}_i &amp;\leftarrow {x_i - \mu_\beta \over \sqrt{\sigma_\beta^2 + \epsilon}} \qquad &amp;\text{ normalize}\\
y_i &amp;\leftarrow \gamma\hat{x}_i^2 + \beta \equiv BN_{\gamma,\beta}\left(x_i \right) \qquad &amp;\text{ scale and shift}
\end{aligned}\]

<p>여기서 파라미터는 $\gamma, \beta$ 가 된다.</p>

<p>배치 정규화 과정은 보통 <em>FC layer</em> 나 <em>Conv. layer</em> 뒤, <em>non-linearity</em> 위치하도록 해준다.</p>

<p>이는 다음의 이점을 가져오게 해준다.</p>

<ul>
  <li>deep network 에서 학습이 쉽게 된다.</li>
  <li>gradient 의 흐름이 향상된다.</li>
  <li>learning rate 가 클 때도 잘 동작하며, 수렴 속도가 빨라진다.</li>
  <li>강한 의존성을 제거하게되어 network 가 초기화에 대해 더 robust 해진다. (나쁜 초기화에 대한 영향에 대해 강해짐)</li>
  <li>train 시 <em>regularization</em> 의 역할을 한다.</li>
  <li>test 시에 overhead 가 없다.(학습된 것을 사용만 함)</li>
</ul>

<p>다만 앞에서 얘기했듯 train 과 test 시 행동이 다르기에 문제가 발생할 수 있다.</p>

<p><em><a href="https://arxiv.org/abs/1502.03167">paper for batch normalization</a></em> « 논문을 참고해보자!</p>

<h2 id="layer-normalization">Layer Normalization</h2>

<p><em>layer normalization</em> 은 직관적으로 이해하기 힘들 수 있는데, 배치 정규화와 다른 점을 간략히 소개하면 아래와 같다.</p>

<blockquote>
  <p>Batch normalization normalizes the input features across the batch dimension.<br />
The key feature of layer normalization is that it normalizes the <strong><em>inputs across the features.</em></strong></p>
</blockquote>

<p>배치 정규화와 다른 부분을 식을 통해 살펴보면 다음과 같다.</p>

<p>Batch Normalization :</p>

\[\begin{aligned}
\mu_j &amp;\leftarrow {1 \over m}\sum_{i=1}^m x_{ij} \\
\sigma_j^2 &amp;\leftarrow {1\over m}\sum_{i=1}^m\left(x_{ij} - \mu_j \right)^2 \\
\hat{x}_{ij} &amp;\leftarrow {x_{ij} - \mu_j \over \sqrt{\sigma_j^2 + \epsilon}}
\end{aligned}\]

<p>Layer Normalization :</p>

\[\begin{aligned}
\mu_i &amp;\leftarrow {1 \over m}\sum_{j=1}^m x_{ij} \\
\sigma_i^2 &amp;\leftarrow {1\over m}\sum_{j=1}^m\left(x_{ij} - \mu_i \right)^2 \\
\hat{x}_{ij} &amp;\leftarrow {x_{ij} - \mu_i \over \sqrt{\sigma_i^2 + \epsilon}}
\end{aligned}\]

<p>식의 차이가 $i, j$ 뿐임을 알 수 있다.<br />
조금 직관적으로 생각해보면, <em>batch normalization</em> 과 <em>layer normalization</em> 은 각각 행/열 에 대한 정규화를 수행하는 것이라 할 수 있는데 다음을 보면 조금 더 와닿을 것이다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image16.png" alt="batch norm and layer norm" /></p>

<p>다시 확인하자면, 배치 정규화는 배치 차원에서 input feature 를 정규화 하는 것이며, 계층 정규화(layer normalization)는 feature 에 대해 input 을 정규화 하는 것이다.</p>

<p><small><em><a href="https://arxiv.org/abs/1607.06450">paper for layer normalization</a></em> « 논문을 참고해 보자</small></p>

<h2 id="instance-normalization">Instance Normalization</h2>

<p><em>instance normalization</em> 은 <em>layer normalization</em> 에서 한 걸음 더 나아간 것이다.<br />
쉽게 생각하면 이전에는 행/열 각각에 대해 정규화를 진행한 것이라면, 이번에는 행/열 <strong>모두</strong> 에 대해 정규화를 진행한 것이다.</p>

<blockquote>
  <p>이는 이미지에 대해서만 가능한 정규화이고, <em>RNN</em> 에서는 사용할 수 없다. <em>style transfer</em> 에 있어서 배치 정규화를 대체해서 좋은 성능을 내는 것으로 보이며 <em>GAN</em> 에서도 사용되었다고 한다.</p>
</blockquote>

<h2 id="group-normalization">Group Normalization</h2>

<p>그룹 정규화(<em>group normalization</em>) 은 채널 그룹에 대한 평균 및 표준 편차를 계산한다.<br />
이는 <em>layer normalization</em> 과 <em>instance normalization</em> 의 조합인데, 모든 채널이 단일 그룹(G=C)이 된다면 <em>layer normalization</em> 이 되고, 각 채널을 다른 그룹에 넣게 될 경우(G=1) <em>instance normalization</em> 이 된다.</p>

<p>그룹 정규화는 <em>ImageNet</em> 에서 batch size 32 인 <em>batch normalization</em> 의 성능에 근접하며, 더 작은 크기에서는 성능이 더 좋게 나타난다.<br />
또한, 높은 해상도의 이미지를 사용하여 물체를 감지(detection)하거나 분할(segmentation)하는 문제는 메모리 문제로 배치 크기를 늘리기 어려운데 이러한 문제에 대해 그룹 정규화는 매우 효과적인 정규화 방법이다.</p>

<p>그룹 정규화가 <em>layer</em> 또는 <em>instance</em> 정규화에 비해 효과적인 이유는 다음과 같다.<br />
<em>layer normalization</em> 의 가정은 평균을 계산할 때 모든 채널이 <strong>동일하게 중요</strong> 하다는 것을 이용한다. 이는 항상 옳을 수 있는 것이 아니다. 이미지의 채널은 완전히 독립적일 수 없는데, 그룹 정규화는 다른 채널에 대해 다른 activation 통계를 계산할 때 모델에 필요한 유연성(flexibility)을 줄 수 있다. 이렇게 다른(근처) 채널의 통계를 이용할 수 있는 것이 바로 그룹 정규화의 이점이 된다.</p>

<p><small><em><a href="https://arxiv.org/abs/1803.08494">paper for group normalization</a></em> « 논문을 참고해 보자</small></p>

<p>각 정규화를 간단하게 시각화 하면 다음과 같다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-training-neural-networks-part-1/image17.png" alt="type of normalization" /></p>

<blockquote>
  <p>위의 그림에서 이미지의 resolution 은 H,W 로 하나의 차원으로 표현되었으며, C 는 channel axis(채널의 개수), N 은 batch axis(배치의 개수) 이다.</p>
</blockquote>

<hr />

<h2 id="summary">Summary</h2>

<p>이번 강의에서 학습한 것을 간단히 요약하면 다음과 같다.</p>

<ul>
  <li>Activation Function(default : ReLU)</li>
  <li>Data Preprocessing(for image : centered(subtract mean))</li>
  <li>Weight Initialization(Xavier/He init)</li>
  <li>Batch Normalization(권장함)</li>
</ul>

<blockquote>
  <p>이 포스트는 스탠포드의 <a href="http://cs231n.stanford.edu">cs231n</a> 7강 강의를 보고 공부 및 정리한 포스트입니다.<br />
잘못된 것이 있을 수 있습니다.<br />
댓글로 알려주시면 감사합니다!</p>
</blockquote>
:ET