I"›9<p><small>ìµœì¢… ìˆ˜ì •ì¼ : 2019-10-11</small></p>

<blockquote>
  <p>ì´ í¬ìŠ¤íŠ¸ëŠ” ìŠ¤íƒ í¬ë“œì˜ <a href="http://cs231n.stanford.edu">cs231n</a> 4ê°• ê°•ì˜ë¥¼ ë³´ê³  ê³µë¶€ ë° ì •ë¦¬í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.<br />
ì˜ëª»ëœ ê²ƒì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br />
ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•©ë‹ˆë‹¤!</p>
</blockquote>

<h1 id="neural-networks">Neural Networks</h1>

<p>ë³¸ê²©ì ìœ¼ë¡œ ë“¤ì–´ê°€ê¸°ì— ì•ì„œ ìš°ë¦¬ëŠ” ì´ì „ì— linear score function ë¥¼ ì•Œì•„ë³´ì•˜ë‹¤.</p>

<p>$f(x, W) = Wx$</p>

<p>ì´ì œ ìš°ë¦¬ëŠ” ê°„ë‹¨í•œ <em>2-layer Neural network</em> ë¥¼ ë³¼ ê²ƒì´ë‹¤.</p>

<p>$f = W_2 \text{max} \left(0, W_1x \right)$</p>

<p>ì˜¤, <em>2-layer neural network</em> ë¼ í•´ì„œ ê±°ì°½í•  ê²ƒ ê°™ì•˜ëŠ”ë° ë³„ê±° ì—†ë„¤? ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.</p>

<p>ë¬¼ë¡  ë³´ê¸°ì—” ê·¸ëŸ´ ìˆ˜ë„ ìˆìœ¼ë‚˜ layer ê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ ë³µì¡í•œ ë¬¸ì œë“¤ì„ ë†€ë¼ìš°ë¦¬ë§Œí¼ ì‹ ê¸°í•˜ê²Œ í•´ê²°í•´ë‚´ëŠ” ê²ƒì„ ë³´ê²Œ ëœë‹¤.<br />
ì—¬ê¸°ì„œ <em>2-layer neural network</em> ëŠ” <em>fully-connected network</em> ë˜ëŠ” <em>MLP(multilayered perceptron)</em> ìœ¼ë¡œë„ ë¶ˆë¦°ë‹¤.</p>

<p>ì—¬ê¸°ì„œ <em>max</em> ëŠ” ì™œ ìˆëŠ” ê²ƒì¼ê¹Œ? ë§Œì•½ì— ì—†ë‹¤ê³  ìƒê°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>

<p>$f = W_2W_1x$</p>

<p>ì´ëŠ” ê·¸ì € ë˜ í•˜ë‚˜ì˜ linear function ì´ ë˜ê²Œ ëœë‹¤. ì¦‰ ì§€ê¸ˆê¹Œì§€ ë‹¤ë¤˜ë˜ ê·¸ê²ƒë“¤ê³¼ ë‹¤ë¥¼ ê²ƒì´ ì—†ëŠ” ê²ƒì´ ëœë‹¤ëŠ” ë§!</p>

<p>ì´ëŸ¬í•œ <em>max</em> ë¥¼ <em>activation function(í™œì„± í•¨ìˆ˜)</em> ì´ë¼ê³  ë¶€ë¥´ê³  ì—¬ê¸°ì—ëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ í•¨ìˆ˜ë“¤ì´ ì¡´ì¬í•œë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image1.png" alt="activation functions" /></p>

<p>ì´ëŸ¬í•œ í™œì„± í•¨ìˆ˜ë“¤ ì¤‘ <em>sigmoid</em> ê°€ ë§ì´ ì“°ì—¬ì™”ìœ¼ë‚˜ í˜„ì¬ëŠ” <em>ReLU</em> ê°€ ê°€ì¥ ê¸°ë³¸ì ìœ¼ë¡œ ì“°ì´ëŠ” ì¶”ì„¸ì´ë‹¤. <small>í™œì„± í•¨ìˆ˜ì— ëŒ€í•œ ì •ë¦¬ëŠ” 7ê°•ì„ ì •ë¦¬í•œ <a href="https://younghk.github.io/cs231n-Training-Neural-Networks-Part-1/#activation-functions">í¬ìŠ¤íŠ¸</a>ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</small></p>

<p>Neural Networks, ìš°ë¦¬ ë§ë¡œ í•˜ë©´ ì‹ ê²½ë§ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¡œ ë„ì‹í™” í•  ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image2.png" alt="3-layer neural network" /></p>

<p>ì´ë ‡ê²Œ ëª¨ë“  node ë“¤ì´ ë‹¤ìŒ layerì™€ ë‹¤ ì—°ê²° ë˜ì–´ ìˆì–´ì„œ <em>fully-connected layer</em> ë¼ê³  ë¶€ë¥´ë©° <em>Python</em> ì½”ë“œ(numpy)ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">randn</span>

<span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">),</span> <span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
<span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span> <span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2000</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">)))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

    <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
    <span class="n">grad_h</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_h</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">))</span>

    <span class="n">w1</span> <span class="o">-=</span> <span class="mf">1e-4</span> <span class="o">*</span> <span class="n">grad_w1</span>
    <span class="n">w2</span> <span class="o">-=</span> <span class="mf">1e-4</span> <span class="o">*</span> <span class="n">grad_w2</span>
</code></pre></div></div>

<p>layer ì˜ ìˆ«ìì— ë”°ë¥¸ ì‹œê°í™”ë¥¼ í•˜ë©´,</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image3.png" alt="layer number graph" /></p>

<p>ë‰´ëŸ°ì´ ë§ì•„ ì§ˆ ìˆ˜ë¡ ë³µì¡í•´ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image4.png" alt="vary lambda graph" /></p>

<p><em>hyperparameter</em> ì˜€ë˜ $\lambda$ ì˜ í¬ê¸°ì— ë”°ë¼ ë¶„ë¥˜ ëª¨ë¸ì˜ ë³µì¡ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë°, $\lambda$ê°€ ì»¤ì§ˆ ìˆ˜ë¡ ì¼ë°˜í™” ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>ì´ëŸ¬í•œ ë°©ë²•ì€ ìƒë¬¼ì˜ ì‹ ê²½ì„¸í¬(neuron) ì—ì„œ ì°©ì•ˆí•œ ê²ƒìœ¼ë¡œ, êµ¬ì¡°ìƒ ë¹„ìŠ·í•œ ëª¨ìŠµì„ ë³´ì¸ë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image5.png" alt="neuron" /></p>

<p><small><small>ë“œëŸ¬ë‚˜ëŠ” ë‚˜ì˜ ì•…í•„^_^</small></small></p>

<p>ì´ì²˜ëŸ¼ ê° ê°€ì¤‘ì¹˜ë“¤ì— ì˜í•´ ê³„ì‚°ëœ ê°’ì„ í™œì„± í•¨ìˆ˜(<em>activation function</em>)ì„ í†µê³¼ ì‹œì¼œì„œ ë‹¤ìŒìœ¼ë¡œ ë„˜ê¸°ëŠ” ë°©ì‹ì´ ê¸°ë³¸ì ì¸ ê°œë…ì´ë‹¤.</p>

<blockquote>
  <p>ê·¸ëŸ¬ë‚˜ ì¸ê³µì‹ ê²½ë§ì´ ì‹¤ì œ ì‹ ê²½ë§ê³¼ ê°™ì§€ëŠ” ì•Šë‹¤.</p>
</blockquote>

<p>ì¬ë°ŒëŠ” ê²ƒì€ ìµœì‹ ì˜ ì—°êµ¬ë‚´ìš©ì„ ë³´ë©´, ìƒë¬¼í•™ì  ë‰´ëŸ°(biological neuron)ì€ ë³µì¡í•œ íŒ¨í„´ì„ ê°€ì§€ê³ ì„œ ì—°ì‚°ì„ í•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•˜ê²Œ ì„ì˜ë¡œ ì—°ê²°ëœ ì¸ê³µì‹ ê²½ë§(neural network)ë„ íš¨ê³¼ê°€ ìˆë‹¤ëŠ” ì ì´ë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image6.png" alt="random connection NN" /></p>

<p>ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì—¬ì „íˆ ìƒë¬¼í•™ì  ë‰´ëŸ°ì€ ì¸ê³µì‹ ê²½ë§ê³¼ ìƒë‹¹íˆ ë‹¤ë¥¸ë°,</p>

<ul>
  <li>ì¢…ë¥˜ê°€ ë‹¤ì–‘í•˜ë‹¤.</li>
  <li>Dentride(ìˆ˜ìƒëŒê¸°)ëŠ” ë³µì¡í•œ ë¹„ì„ í˜•(non-linear) ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤.</li>
  <li>Synapse(ì‹œëƒ…ìŠ¤)ëŠ” í•˜ë‚˜ì˜ weightë¥¼ ê°€ì§„ ì‹œìŠ¤í…œì´ ì•„ë‹Œ ë³µì¡í•œ ë¹„ì„ í˜• ë™ì  ì‹œìŠ¤í…œ(non-linear dynamical system)ì´ë‹¤.</li>
</ul>

<p>ë“±ë“±ì˜ ì°¨ì´ê°€ ìˆë‹¤. <small>[Dendritic Computation. London and Hausser]</small></p>

<p>ì, ì§€ê¸ˆê¹Œì§€ ë°°ì› ë˜ ê²ƒì„ ê°„ëµí•˜ê²Œ ì •ë¦¬í•´ë³¸ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<p>$s = f(x;W_1, W_2) = W_2 \text{max} \left(0, W_1x \right)$ <small>â€ƒâ€ƒNonlinear score function</small><br />
$L_i = \sum_{j \ne y_i} \text{max} \left(0, s_j - s_{y_i} + 1 \right)$ <small>â€ƒâ€ƒâ€ƒâ€ƒÂ SVM Loss</small><br />
$R\left(W\right) = \sum_kW_k^2$ <small>â€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒâ€ƒRegularization</small><br />
$L = {1 \over N} \sum_{i=1}^N L_i + \lambda R\left(W_1\right) + \lambda R\left(W_2\right)$ â€ƒ<small>Total loss(data loss + regularization)</small></p>

<p>ì—¬ê¸°ì„œ ì–´ë–»ê²Œ gradient ë¥¼ êµ¬í•´ì•¼í• ê¹Œ?</p>

<p>ì¼ì¼íˆ ì†ìœ¼ë¡œ ë¯¸ë¶„ì„ í•˜ì—¬ êµ¬í•´ì•¼í•˜ëŠ” ê²ƒì„ê¹Œ? í•œë‹¤ê³  í•´ë„ ë§Œì•½ SVM loss ì—ì„œ Softmax ë¡œ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´ ë˜ ë‹¤ì‹œ ì¼ì¼íˆ êµ¬í•´ì•¼í• ê¹Œ?</p>

<p>í•œ ëˆˆì— ë´ë„ ë³„ë¡œ ì¢‹ì§€ ëª»í•œ ê³„íšì¸ ê²ƒì´ ë³´ì¸ë‹¤.<small>(ë„ˆëŠ” ë‹¤ ê³„íšì´ ìˆêµ¬ë‚˜!)</small></p>

<p>ìš°ë¦¬ëŠ” ì´ê²ƒë³´ë‹¤ ë” ë‚˜ì€ ì—­ì „íŒŒ(Backpropagation)ë¥¼ ì´ìš©í•œ updateë¥¼ í•˜ê²Œ ëœë‹¤.</p>

<h2 id="backpropagation">Backpropagation</h2>

<p>ì˜ˆì œë¥¼ í†µí•´ ê°„ë‹¨í•˜ê²Œ backpropagation ê³¼ì •ì„ ë”°ë¼ê°€ë³´ì.</p>

<p>$f(x,y,z) = (x + y)z$ â€ƒ â€ƒ ($x=-2 \text{ , }y=5 \text{ , } z=-4$)</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image7.png" alt="backpropagation step 1" /></p>

<p>$q=x+y$ â€ƒ${\partial q \over \partial x} = 1$ â€ƒ ${\partial q \over \partial y} = 1$<br />
$f=qz$ â€ƒâ€ƒ ${\partial f \over \partial q} = z$ â€ƒ ${\partial f \over \partial z} = q$</p>

<p>ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ ê°ê°ì˜ í¸ë¯¸ë¶„ ê°’(gradient) ë¥¼ êµ¬í•´ë³´ì.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image8.png" alt="backpropagation step 2" /></p>

<p>ìš°ì„  ìëª…í•˜ê²Œë„ ${\partial f \over \partial f} = 1$ ì„ì„ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image9.png" alt="backpropagation step 3" /></p>

<p>ë‹¤ìŒì€ $z$ ì— ëŒ€í•œ $f$ì˜ ìˆœê°„ë³€í™”ìœ¨ì¸ë°, ì´ëŠ”<br />
$q=x+y$ â€ƒ${\partial q \over \partial x} = 1$ â€ƒ ${\partial q \over \partial y} = 1$<br />
$f=qz$ â€ƒâ€ƒ ${\partial f \over \partial q} = z$ â€ƒ ${\partial f \over \partial z} = q$<br />
ì—ì„œ ë³¼ìˆ˜ ìˆë“¯ì´ $q$ ì´ë‹¤.
ì¦‰, ${\partial f \over \partial z} = q = 3$ ì´ ë˜ê²Œ ëœë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image10.png" alt="backpropagation step 4" /></p>

<p>${\partial f \over \partial q} = z = -4$ ì—­ì‹œ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image11.png" alt="backpropagation step 5" /></p>

<p>ì´ì œ ${\partial f \over \partial y}$ ë¥¼ êµ¬í•´ì•¼í•˜ëŠ”ë°, ì´ëŠ” ë°”ë¡œ êµ¬í•  ìˆ˜ ì—†ìœ¼ë‹ˆ <strong><em>chain rule</em></strong> ì„ í†µí•´ ê³„ì‚°í•´ì¤€ë‹¤.<br />
ì—¬ê¸°ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ${\partial f \over \partial q}$ ë¥¼ <em>upstream<small>(global)</small> gradient</em> , ${\partial q \over \partial y}$ ë¥¼ <em>local gradient</em> ë¼ê³  ë¶€ë¥¸ë‹¤.<br />
<em>chain rule</em> ì— ì˜í•´ ${\partial f \over \partial y} = z \times 1 = -4$ ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image12.png" alt="backpropagation step 6" /></p>

<p>ë§ˆì°¬ê°€ì§€ë¡œ <em>chain rule</em> ì— ì˜í•´ ${\partial f \over \partial x} = z \times 1 = -4$ ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ì´ëŸ¬í•œ ì¼ë ¨ì˜ ê³¼ì •ì„ ê±°ì³ì„œ ê³„ì‚°í•œ gradient ê°€ í•™ìŠµë˜ê²Œ ë˜ëŠ”ë°, ì´ê²ƒì´ ë°”ë¡œ <strong><em>Backpropagation</em></strong> ì´ë‹¤.</p>

<p>ì´ë¥¼ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image13.png" alt="backpropagation step 7" /></p>

<p>gradient flow ì— ìˆì–´ì„œ ê° gate ë“¤ì˜ íŠ¹ì„±ì„ ì •ë¦¬í•  ìˆ˜ ìˆëŠ”ë° ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image14.png" alt="gradient flow characteristic" /></p>

<hr />

<h2 id="backpropagation-with-vectors">Backpropagation with Vectors</h2>

<p>ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ scalar ê°’ì— ëŒ€í•œ ì—­ì „íŒŒ ê³¼ì •ì„ í•™ìŠµí–ˆë‹¤.</p>

<p>ê·¸ë ‡ë‹¤ë©´ vector ì— ëŒ€í•œ ì—­ì „íŒŒëŠ” ì–´ë–»ê²Œ ì´ë£¨ì–´ì§ˆê¹Œ?</p>

<p>ìš°ì„  vector to vectorì˜ ë¯¸ë¶„ê°’(derivative)ì€ <strong>Jacobian</strong> ì„ì„ ë– ì˜¬ë ¤ë³´ì. ìˆ˜ì‹ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<p>${\partial y \over \partial x} \in \mathbb{R}^{N \times M}$ â€ƒ $\left({\partial y \over \partial x} \right)_{n,m} = {\partial y_m \over \partial x_n}$</p>

<p>vector to scalarì˜ derivativeëŠ” <em>gradient</em> ì´ê³  ìˆ˜ì‹ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<p>${\partial y \over \partial x} \in \mathbb{R}^N \qquad \ \; \left( {\partial y \over \partial x} \right)_n = {\partial y \over \partial x_n}$</p>

<p>ì´ë¥¼ ë§ˆìŒ ì†ì— ê°„ì§í•˜ë©° vector ì˜ ì—­ì „íŒŒ ê³¼ì •ì„ ì‚´í´ë³´ì.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image15.png" alt="backpropagation with vector step 1" /></p>

<p><em>Upstream gradient</em> ${\partial L \over \partial z}$ ê°€ $D_z$ ë¼ëŠ” derivative matrix ë¼ í•˜ê³ ,<br />
$x$ ì™€ $y$ ì— ëŒ€í•´ì„œë„ ê°ê° derivative matrix ë¥¼ $D_x$, $D_y$ ë¼ê³  í•˜ë©´,</p>

<p>${\partial z \over \partial x}$ ëŠ” $D_x \times D_z$,<br />
${\partial z \over \partial y}$ ëŠ” $D_y \times D_z$,</p>

<p>ì¦‰ <em>Jacobian matrices</em> ë¡œ ë³¼ ìˆ˜ ìˆê³ , ë„ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ì•„ë˜ì™€ ê°™ê²Œ ëœë‹¤.</p>

<p><img src="/assets/images/2019-10-11---cs231n-neural-networks-and-backpropagation/image16.png" alt="backpropagation with vector step 2" /></p>

<p><small>vector ì˜ backpropagation ì€ ë‚˜ì¤‘ì— ê¸°íšŒê°€ ë˜ë©´ ë” ì •ë¦¬í•´ì•¼ê² ë‹¤..</small></p>
:ET