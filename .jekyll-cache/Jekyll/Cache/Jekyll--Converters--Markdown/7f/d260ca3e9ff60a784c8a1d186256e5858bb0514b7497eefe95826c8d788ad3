I"e<h2 id="graph-pooling">Graph Pooling</h2>

<ul>
  <li>Global Pooling : summarize graph into fixed-size representation<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image1.png" alt="global pooling" /></li>
  <li>Hierarchical Pooling : Learn hierarchical representation<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image2.png" alt="hierarchical pooling" /></li>
</ul>

<h3 id="global-pooling">Global Pooling</h3>

<ul>
  <li>Simple Readout : Average / Max / Min / …<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image3.png" alt="simple readout" /></li>
  <li>Neural Networks for Readout : GG-NN / Set2Set / SortPool<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image4.png" alt="nn for readout" /><br />
SortPool 은 각 node 에 대해 feature 별로 sorting 한 후 몇 개의 node 만 선택하는 방법</li>
</ul>

<h3 id="hierarchical-pooling">Hierarchical Pooling</h3>

<ul>
  <li>DiffPool(Differentiable Pooling) : softly assign node from bottom to higher<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image5.png" alt="diffpool" /></li>
  <li>gPool(Graph Pooling)<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image6.png" alt="gpool" /></li>
  <li>SAGPool(Self-Attention Graph Pooling)<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image7.png" alt="sagpool" />
    <ul>
      <li>여기서 $\tilde{A} \in \mathbb{R}^{N\times N}$ 는 Adjacency matrix</li>
      <li>$\tilde{D} \in \mathbb{R}^{N\times N}$ 는 degree matrix of $\tilde{A}$</li>
      <li>$X \in \mathbb{R}^{N\times F}$ 는 N 개의 node 의 F dimensional feature 를 가진 input graph feature</li>
      <li>SAGPool 에서 parameter 는 $\Theta_{att} \in \mathbb{R}^{F \times 1}$ 뿐이다.</li>
      <li>pooling 결과는 graph feature 와 topology 에 기반하며, graph convolution 을 이용해 self-attention score 를 얻는다.</li>
      <li>node selection method 은 다양한 크기와 구조의 입력에 대해서도 입력 그래프의 노드 일부를 유지할 수 있도록 함.</li>
      <li></li>
    </ul>
  </li>
  <li>EdgePool<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image8.png" alt="edgepool" /></li>
</ul>

<h2 id="scene-graph-learning">Scene graph learning</h2>

<p><img src="/assets/images/2019-12-14---graph-neural-networks-2/image9.png" alt="scene graph learning pipeline" /></p>

<p>Visual relationships of an image as a graph :<br />
Predicate detection &lt;=&gt; Object detection</p>

<p>Multi-relational tensor 를 사용함.<br />
[[Object, Predicate, Object ]] 로 쌍을 만들어 냄</p>

<p>Ground Truth 를 어떻게 해야하는지에 대한 고민이 많이 필요하다.<br />
(이렇게 저렇게 여러가지 방향으로 해석이 가능하므로)</p>

<h2 id="graph-transformer-networksgtn">Graph Transformer Networks(GTN)</h2>

<ul>
  <li>
    <p>Spatial Transformer Networks<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image10.png" alt="spatial transformer network" /></p>
  </li>
  <li>
    <p>meta-path : multi-hop 관계를 새로운 path 로 보고 새롭게 네트워크를 구성할 수 있게 함.<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image11.png" alt="meta path" /></p>
  </li>
  <li>
    <p>graph transformer layer : learn a soft selection of edge types and composite relations for generating useful multi-hop connections(meta-paths)<br />
<img src="/assets/images/2019-12-14---graph-neural-networks-2/image12.png" alt="graph transformer layer" />
위의 예제는 2-hop relationship 을 보고 있는 것이다. 마지막에는 meta-path 만 나타난다.</p>
  </li>
</ul>

<p>이 때, meta-path 의 길이가 늘어날 수록(hop 이 많아질 수록) 문제점이 발생할 수 있다.</p>

<blockquote>
  <p>경로가 중복이 되는 문제일까?</p>
</blockquote>
:ET