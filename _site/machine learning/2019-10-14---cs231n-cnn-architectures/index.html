<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.3 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[cs231n-lec9] CNN Architectures - Cheong’s log</title>
<meta name="description" content="cs231n lec 9 강을 보고 정리한 글입니다. CNN 구조에 대해 학습합니다.">


  <meta name="author" content="Younghk">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Cheong's log">
<meta property="og:title" content="[cs231n-lec9] CNN Architectures">
<meta property="og:url" content="http://localhost:4000/machine%20learning/2019-10-14---cs231n-cnn-architectures/">


  <meta property="og:description" content="cs231n lec 9 강을 보고 정리한 글입니다. CNN 구조에 대해 학습합니다.">







  <meta property="article:published_time" content="2019-10-14T03:04:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/machine%20learning/2019-10-14---cs231n-cnn-architectures/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Cheong",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Cheong's log Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--post">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/profile.png" alt=""></a>
        
        <a class="site-title" href="/">
          Cheong's log
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Younghk</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>오직 부끄러워야 할 것은 열심히 살지 않은 어제의 나이다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, KR</span>
        </li>
      

      
        
          
        
          
            <li><a href="https://younghk.github.io" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/younghk/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/younghoon-kim-03a667191/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">[cs231n-lec9] CNN Architectures</h1>
    
    <p><small>최종 수정일 : 2019-10-15</small></p>

<p>우리는 이전 두 개의 강의(를 정리한 <a href="../cs231n-training-neural-networks-part-1/">포스트1</a>, <a href="../cs231n-training-neural-networks-part-2/">포스트2</a>)에서 Neural Network 를 학습시키는 방법과 그 과정에 대해서 학습했었다.</p>

<p>여기서 한 가지 더 NN 을 학습시키는 방법이 있는데, 바로 <strong>Transfer Learning</strong> 이다.</p>

<h2 id="transfer-learning">Transfer Learning</h2>

<p>전이 학습(<em>Transfer Learning</em>)은 쉽게 생각해서 다음과 같다.<br />
이전에 학습한 결과를 이용해 새로운 모델(또는 데이터셋)에 적용하는 방법이다.</p>

<p>개인이 Neural Network 를 학습시킨다고 하자. Neural Network 는 학습을 위한 연산량이 아주 많은데 이를 개인의 컴퓨팅 리소스로 감당하기에는 쉽지 않은 일이다.</p>

<p>이를 극복하기 위해 이미 학습된 것을 바탕으로 다른 개별 문제에 적합하게 일부(끝 부분 이라던가)만 다시 학습시키는 방법이 바로 전이 학습이다.</p>

<p>이는 내가 풀고자 하는 문제의 dataset 이 적을 경우 좋은 학습 방법이며, 다음과 같이 경우를 나눠서 생각해 볼 수 있다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image1.png" alt="transfer learning" /></p>

<p>각 경우에 대해 표로 정리해보면 아래와 같이 정리된다.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center"> </th>
      <th style="text-align: center">비슷한 데이터셋</th>
      <th style="text-align: center">다른 데이터셋</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">적은 데이터</td>
      <td style="text-align: center">Linear Classifier를 top layer 에 적용</td>
      <td style="text-align: center">적용하기 힘들다. linear classifier 를 다른 stage 에서 시도해 보거나 해야한다.</td>
    </tr>
    <tr>
      <td style="text-align: center">꽤 많은 데이터</td>
      <td style="text-align: center">몇몇 layer 에 finetuning</td>
      <td style="text-align: center">많은 레이어에 finetuning</td>
    </tr>
  </tbody>
</table>

<p>보통 ImageNet 으로 학습된 것을 이용하는데 ImageNet 은 1500만 장 이상의 데이터가 이미 축적된 아주 거대한 데이터셋이다.</p>

<p>이러한 기법은 아주 권장되며 널리 사용되고 있는데, 처음부터 모든 것을 training 시키는 것은 힘들기만 할 뿐 아니라 시간적인 측면에서 비효율적이다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image2.png" alt="cnn transfer learning rnn" /></p>

<p>위의 그림은 ImageNet 과 word2vec 으로 pretrained 된 것을 이용한 방법이다.</p>

<p>그러나 <em>transfer learning</em> 이 항상 필요한 것은 아니라는 내용의 연구 결과도 발표(<a href="https://arxiv.org/abs/1811.08883">관련 논문</a>)되었는데, 여전히 작은 데이터셋에 대해서는 효과적인 기법이 되겠다.</p>

<h2 id="cnn-architecture">CNN Architecture</h2>

<p>이제 다양한 CNN 의 구조에 대해서 살펴볼 것이다.</p>

<p>우선 LeCun 이 1998년에 발표한 LeNet-5 를 잠깐 상기하고 넘어가보자.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image3.png" alt="lenet-5" /></p>

<p>5*5 Conv filter 를 이용한 <em>LeNet-5</em> 는 필기체 인식에 있어서 좋은 성능을 냈었다.<br />
그 구조는 CONV-POOL-CONV-POOL-FC-FC 를 가지고 있었다.</p>

<h3 id="alexnet">AlexNet</h3>

<p>처음 살펴볼 <em>AlexNet</em> 은 CNN 으로 눈에 띌 만한 성과(대회에서 1등)를 낸 첫 CNN 이다.<br />
구조적으로 <em>LeNet-5</em> 와 비슷하나 조금 더 많은 layer 를 가진다는 차이가 있다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image4.png" alt="AlexNet" /></p>

<p>위의 도식은 <em>AlexNet</em> 의 실제 구현 구조이다.<br />
각 레이어와 레이어에서 처리되는 과정을 간략히 표현하면 다음과 같다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image7.png" alt="AlexNet arthitecture" /></p>

<p>몇 가지 확인을 위해 질문에 답을 해보자.</p>

<ol>
  <li>첫 번째 layer(CONV1) 를 거쳤을 때의 output volume?
    <blockquote>
      <p>stride 가 4이기 때문에 한 쪽(width 또는 height)에 적용되는 filter 는 ${227-11 \over 4} + 1 = 55$ 가 된다. 따라서 $55 \times 55 \times 96$ 이 된다.(총 96개의 필터를 적용)</p>
    </blockquote>
  </li>
  <li>첫 번째 layer(CONV1) 의 전체 parameter 의 수는?
    <blockquote>
      <p>필터의 크기가 $11 \times 11$ 이므로 채널(depth)이 3임에 주의하면 답은 총 $(11\times11\times3)\times96$ 임을 알 수 있다.</p>
    </blockquote>
  </li>
  <li>두 번째 layer(POOL1) 을 거쳤을 때의 output volume?
    <blockquote>
      <p>1의 문제와 동일하게 접근하면 $27\times27\times96$ 임을 알 수 있다.(depth = 96)</p>
    </blockquote>
  </li>
  <li>두 번째 layer(POOL1) 의 전체 parameter 의 수는?
    <blockquote>
      <p>이 질문을 받았을 때, “이게 뭔 소리야?” 했다면 정상이다. pooling layer 는 해당 영역을 그냥 보고 선택하게 되기 때문에 학습을 기다리는 parameter 가 없다. 즉, <strong>0</strong>이다.</p>
    </blockquote>
  </li>
</ol>

<p><em>AlexNet</em> 은 다음과 같은 특징을 가진다.</p>

<ul>
  <li>처음으로 <em>ReLU</em> 를 도입했다.</li>
  <li>Norm layer 를 사용했다.(이제는 쓰지 않음)</li>
  <li>data augmentation 을 많이 사용했다.(논문 확인)</li>
  <li>dropout 을 도입했으며 0.5의 값을 사용</li>
  <li>batch size = 128</li>
  <li><em>SGD + Momentum</em> 을 최적화 알고리즘으로 사용했으며, momentum = 0.9</li>
  <li>learning rate = 1e-2 로 초기화 했으며 val accuracy 가 평탄해지는 구간에 수동적으로(manually) 10만큼 나눠줬다.</li>
  <li>L2 weight decay 를 적용했으며, 5e-4 로 사용했다.</li>
  <li>최종 결과에서 좋은 결과를 얻기 위해 7개의 CNN 모델을 앙상블 시켜서 18.2% -&gt; 15.4% 의 성능 향상을 보였다.</li>
</ul>

<p>여기서 도식과 구조를 보면 조금 다른 점을 알 수 있는데,</p>

<ol>
  <li>구조는 227*227 이나 도식에서는 224*224 이다. 이는 227 이 맞는 수치이다.</li>
  <li>도식에서 layer 가 2개로 분할이 되어서 진행이 되는데, 이는 당시 GPU 의 성능이 좋지 못해서(GTX 580, 3gb mem) 두 부분으로 나눠서 각각 GPU 를 하나씩 사용해서 학습할 수 있도록 하였기 때문이다.</li>
</ol>

<p>2번에 대한 부연 설명을 조금 더 하자면 CONV1, CONV2, CONV4, CONV5 는 동일한 GPU 의 feature map 과 연결이 되어 있다.<br />
그러나 CONV3, FC6, FC7, FC8 은 바로 전 단계의 모든 feature map 과 연결(즉 2개로 분할된 각각의 레이어와 모두 communication)하여 진행했다.</p>

<p>이러한 구조의 <em>AlexNet</em> 은 ILSVRC(ImageNet Large Scale Visual Recognition Challenge)에서 CNN 구조를 가진 모델 중 처음으로 우승하게 되었고, 이미지 인식 분야에서 CNN 의 시대가 오게 됨을 알리게 되었다.<br />
이 <em>AlexNet</em> 의 연구 성과에 힘입어 매년 이미지 인식 모델의 성능은 눈에 띄게 발전하게 된다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image8.png" alt="zfnet" /></p>

<p>이듬해 <em>AlexNet</em> 을 조금 더 최적화한 <em>ZFNet</em> 이 발표되는데,<br />
CONV1 : 11*11 stride 4 -&gt; 7*7 stride 2<br />
CONV3,4,5 : 384, 384, 256개의 필터 -&gt; 512, 1024, 512개의 필터<br />
로 변경된 모델이었다.</p>

<h3 id="vgg">VGG</h3>

<p><em>AlexNet</em> 의 연구 성과를 바탕으로 2년 뒤, <em>VGGNet</em> 이 발표된다. 이 모델은 ILSVRC 2014 에서 2위를 차지<small>(localization에서는 우승)</small>한 모델이다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image9.png" alt="vggnet" /></p>

<p>기존의 <em>AlexNet</em> 보다 더 깊어진 network 를 구성하며, 더 작은 필터를 사용하는 것이 주요 골자이다.</p>

<p>특히 모든 CONV layer 에서 $3\times3$ stride 1, pad 1 을 유지하는 점과,<br />
MAX POOL 에서 $2\times2$ stride 2 로 유지하는 통일성을 보인다.</p>

<p>즉 더 심플하면서도 성능은 더 좋은 결과를 보이는 모델이다.<br />
이는 11.7% 를 기록한 <em>ZFNet</em> (2013) 에서 7.3% 의 성능을 보여준다.</p>

<p>여기서 우리는 <strong>“왜 더 작은 필터를 쓰는가?”</strong> 에 대한 의문이 들 수 있다.<br />
중요한 통찰을 얻을 수 있는 질문인데, <strong><em>3개의 3*3 CONV layer 를 쌓아올리면 7*7 CONV layer 하나를 적용한 것과 같은 효과를 지닌 receptive field</em></strong> 를 얻게 된다.</p>

<blockquote>
  <p>첫 레이어에서는 3*3 의 정보만을 얻게 된다. 그러나 두 번째 레이어가 적용이 되면, 가장자리에서 2칸씩 확장이 되어 5*5 의 정보를 얻게 되고, 마찬가지로 세 번째 레이어가 적용이 되면 7*7 의 정보를 얻게 되는 것이다.</p>
</blockquote>

<p>이렇게 더 깊어지면서도 더 많은 non-linearity 를 처리할 수 있게 되는 것이다.</p>

<p>그렇지만 왜 7*7 필터를 바로 적용하지 않는가라고 되물을 수 있다.<br />
이는 computational cost 를 고려한 설계이기 때문인데, 레어이당 C개의 채널에 대한 파라미터의 개수를 계산한 $3\times3^2C^2$ 와 $7^2C^2$ 를 비교해보면 쉽게 이해할 수 있다.</p>

<p>각 레이어에 대한 메모리와 파라미터를 계산해보면 아래와 같다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image10.png" alt="vggnet compute cost" /></p>

<p>이미지당 대략 96MB 의 메모리가 필요하다.<small>(이마저도 forward pass 만 고려한 것이고, backward pass 까지 하면 2배)</small><br />
자세히 살펴보면 CONV layer 에서 많은 메모리가 필요하며, FC layer 에서 대부분의 파라미터가 필요하다는 것을 알 수 있다.</p>

<p>각 레이어에 대해서 conv1-1, conv1-2, conv2-1, … 이렇게 부르기도 한다.</p>

<p>정리하면 <em>VGGNet</em> 은 다음과 같은 특징을 가진다.</p>

<ul>
  <li>ILSVRC’14 에서 classification 2위, localization 1위</li>
  <li>Krizhevsky 의 2012년의 연구와 비슷한 학습 과정</li>
  <li>local response normalization(LRN)이 없음</li>
  <li>VGG16 또는 VGG19 가 있다(레이어의 차이, VGG19가 아주 조금 더 나으나 더 많은 메모리를 사용한다)</li>
  <li>더 좋은 결과를 내기 위해 앙상블 사용</li>
  <li>FC7(위에서 두 번째 FC layer)이 feature 를 잘 표현함(good feature representation)</li>
</ul>

<h3 id="googlenet">GoogLeNet</h3>

<p>ILSVRC’14 에서 <em>VGGNet</em> 은 괄목할만한 성능의 향상을 가져왔음에도 불구하고 classification 에서 2위를 차지했는데, 이번에는 1위를 차지한 모델에 대해서 알아보자.</p>

<p>바로 구글의 <em>GoogLeNet</em> 이다.<br />
총 22개의 레이어를 사용했는데 network 가 더 깊어지면서도 계산 효율이 증대되었다.</p>

<p>이는 <strong>Inception module</strong> 을 쌓아올리면서 구현되었는데, <em>GoogLeNet</em> 의 핵심 <em>Inception module</em> 에 대해 알아보자.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image11.png" alt="inception module" /></p>

<p><em>Inception module</em> 은 local network topology 을 아주 잘 설계한 예로, 이 모듈의 끝에 다시 모듈을 쌓아 올리는 방식으로 인공신경망의 구조를 이루게 한다.</p>

<p>왜 이러한 구조를 가지게 되었는지 하나씩 살펴보자.<br />
다음은 naive inception modeul 이다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image12.png" alt="naive inception module" /></p>

<p>이는 1*1, 3*3, 5*5 CONV layer 와 3*3 MAX POOL layer 를 평행하게 적용하여 최종적으로 모두 depth-wise하게 이어붙여(concatenation) 출력을 만들어내는 구조이다.</p>

<p>이렇게 했을 때의 문제점은 바로 computational cost 가 비싸다는 점이다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image13.png" alt="naive inception module 2" /></p>

<p>해당 모듈에 28*28*256 의 입력이 들어오게 된다면 위와 같은 CONV layer 를 통과하게 되었을 때 computational cost 가 어떻게 되는지 계산해보자.<br />
1*1*128 CONV layer 를 통과하게 된다면 28*28*128 의 output 이 나오게 된다. 마찬가지로 모두 다 계산해보면 다음과 같다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image14.png" alt="naive inception model 3" /></p>

<p>각 필터들 depth-wise 하게 이어붙이면 결국에는 $28\times28\times(128+192+96+256)=28\times28\times672$ 의 output 이 되게 된다.<br />
뿐만 아니라 연산량(operations)는 무려,<br />
[1*1 conv, 128] = $28 \times 28 \times 128 \times 1 \times 1\times256$<br />
[3*3 conv, 192] = $28 \times 28 \times 192 \times 3 \times 3\times256$<br />
[5*5 conv, 96] = $28 \times 28 \times 96 \times 5 \times 5\times256$<br />
가 되어 최종적으로 <strong>854M</strong> 만큼이 된다.</p>

<p>이는 너무나도 비싼 연산이 되는데, pooling layer 도 feature 의 depth 를 유지하기 때문에 결국 concatenation 을 하게 되면 depth 는 매 레이어마다 증가할 수 밖에 없게 된다.</p>

<p>이를 해결하기 위해 <strong>bottleneck</strong> layer 를 도입하게 된다.<br />
이 <em>bottleneck layer</em> 는 1 by 1 CONV layer 인데, 이전 강의에서도 간략히 짚고 넘어갔지만 한 번 더 쉽게 설명하면 다음과 같다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image16.png" alt="1 by 1 conv" /></p>

<p>56*56*64 의 input 에 대해 1*1 CONV layer 인 32개의 필터를 적용시키면 56*56 의 공간은 유지되면서 채널(depth)이 32로 줄어드는 효과를 얻게 된다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image17.png" alt="inception module with bottleneck layer" /></p>

<p>이러한 <em>bottleneck layer</em> 를 다음과 같이 적용시키므로써 dimension reduction(차원 축소) 을 진행하는 <em>inception module</em> 은 다음과 같이 아주 많은 계산 상의 이점을 가져오게 한다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image18.png" alt="inception module with dimension reduction" /></p>

<p>최종적으로 filter concatenation 을 하게 되면 28*28*480 으로 약 2/3 로 줄어들은 것을 확인할 수 있으며, 연산량을 모두 계산하면 <strong>358M</strong> 이 되어 절반 정도로 줄어들은 것을 볼 수 있다.<br />
<small>
[1*1 conv, 64] = $28 \times 28 \times 64 \times 1 \times 1\times256$<br />
[1*1 conv, 64] = $28 \times 28 \times 64 \times 1 \times 1\times256$<br />
[1*1 conv, 128] = $28 \times 28 \times 128 \times 1 \times 1\times256$<br />
[3*3 conv, 192] = $28 \times 28 \times 192 \times 3 \times 3\times64$<br />
[5*5 conv, 96] = $28 \times 28 \times 96 \times 5 \times 5\times64$<br />
[1*1 conv, 64] = $28 \times 28 \times 64 \times 1 \times 1\times256$<br />
</small></p>

<p>pooling layer 역시 depth 가 낮아지는 것 또한 확인할 수 있다.</p>

<p>이러한 <em>GoogLeNet</em> 의 전체 구조는 다음과 같다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image19.png" alt="googlenet" /></p>

<p>구조를 보았을 때 왼쪽 끝에서부터 첫 inception module 전 까지를 ‘stem network’,<br />
그 다음 마지막 inception module 까지를 ‘stacked inception modules’,<br />
마지막 4개의 layer 가 ‘classifier output’ 의 부분이다.</p>

<p>이 때, ‘classifier output’ 에서 FC layer 가 없음을 확인할 수 있다.<br />
이는 계산이 많은 계층이기에 삭제되었다.</p>

<p>중간에 두 개의 Auxiliary classification output 을 확인할 수 있는데, deep 하게 내려가기 전에 출력을 내보내는 mini network 이다. 이런 구조를 가졌을 때 classification 하는데 있어서 더 도움을 주게 된다. 이는 layer 가 깊어질 수록 gradient 가 없어질 수도 있기 때문에 이를 보완하는 효과를 가지고 오게 된다.</p>

<p>이는 구글에서 다양한 시도 끝에 내린 결론적인 구조이고, 많은 시행착오가 있었을 것이다.</p>

<p><em>GoogLeNet</em> 의 특징을 정리하면 다음과 같다.</p>

<ul>
  <li>22개의 레이어를 가진다(중간의 mini network 는 세지 않는다)</li>
  <li>효율적인 inception module 을 쌓아 올리므로써 구현된다.</li>
  <li>multiple FC layer 로 인한 비싼 계산 코스트를 방지하기 위해 FC layer 를 삭제했다.</li>
  <li>AlexNet 보다 12배나 적은 파라미터 수를 갖는다.</li>
  <li>ILSVRC’14 에서 classification 분야 1위(6.7%)</li>
</ul>

<p>지금까지의 연구 성과를 그래프로 표현하면 다음과 같다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image20.png" alt="ilsvrc winners" /></p>

<h3 id="resnet">ResNet</h3>

<p>2015년, depth 에 아주 큰 변화가 나타나게 된다. 종전 22개의 layer 에서 무려 152개의 layer 로 늘어나게 되었는데, 마이크로소프트의 (2019년 현재는 페이스북) Kaiming He 가 개발한 ResNet 이 바로 그 주인공이다.</p>

<p><em>ResNet</em> 을 한 문장으로 정리하면 다음과 같다.</p>

<blockquote>
  <p>Very deep networks using residual connections<br />
residual(나머지), 무슨 구조를 가지기에 나머지라는 것이 나온 것일까?</p>
</blockquote>

<p>우선 신경망을 깊이 만드는 것을 생각하자.<br />
연구자들은 신경망의 깊이가 깊어지면 성능이 증가할 것이라고 믿었다. 왜냐하면 더 복잡한 정보를 처리할 것이라 생각했기 때문이다.</p>

<p>그렇다면 plain CNN 에 계속해서 layer 를 쌓으면 어떻게 될까?</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image21.png" alt="error at deeper layer" /></p>

<p>위의 그래프는 VGGNet 에서 레이어를 추가했을 때의 training 과 test 에서의 에러를 나타낸 그래프이다.<br />
이것이 나타내는 바는 무엇일까?</p>

<p>overfitting 이라고 생각할 수도 있으나 training 에서도 에러가 증가하는 것을 보면 전체적인 성능이 낮아지는, underfitting 이 일어나고 있는 것을 볼 수 있다.</p>

<p>He 는 이러한 현상이 일어나는 이유를 layer 가 최적화가 되어 있지 않기 때문이라고 생각했다.<br />
일반적으로 더 복잡한 정보를 처리하면 성능이 올라가야하지만 그렇지 못했기 때문이다.</p>

<p>구조적으로 이를 해결하는 방법은 얕은 레이어를 가진 모델에서 학습된 것을 복사한 다음 추가적인 레이어를 identity mapping 을 위해 놓는 것이다.</p>

<p><em>ResNet</em> 에서는 위의 문제점에 대한 해결책으로 network layer 를 residual mapping 에 맞추도록 컨셉을 변경한 것이다.</p>

<p>기존의 방식은 직접적으로 layer 가 요구되는 답을 학습하도록 설계되어 있었다는 점을 상기해보자.<br />
즉, 기존의 layer 는 출력값 $H(x)$ 를 최적화 하는 방식이었는데, <em>ResNet</em> 의 컨셉은 입력값 $x$ 와 출력값 $H(x)$ 의 차이 $F(x)$ 를 최소화하는 것이 layer 의 목표가 된 것이다. 즉 $F(x) = 0$ 이 되도록 layer 를 다르게 설계하게 되는 것이다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image22.png" alt="resnet residual block" /></p>

<p>이렇게 했을 때 어떤 이점이 있을까?</p>

<p>이는 이론적으로 레이어가 학습하는데 더 용이한데, 만일 어떤 H(x) 가 $H(x)=x$ 를 만족하도록 학습해야했다고 하자. <em>ResNet</em> 에서는 $F(x)=0$ 이 되도록 학습하게 되면 어떤 것이 쉽게 학습하게 되는 결과를 가져오게 될까?<br />
당연히 0 쪽이 더 쉬울 것이다.</p>

<p>이러한 개념을 이용해 <em>ResNet</em> 은 다음과 같은 구조를 갖는다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image23.png" alt="resnet architecture" /></p>

<ul>
  <li><strong>residual block</strong> 을 쌓아 올리는 것이 핵심이며, 모든 residual block 은 2개의 3*3 CONV layer 를 갖는다.</li>
  <li>주기적으로 두 배의 filter 를 주고 stride 2 를 이용해 공간적인 downsampling 을 진행하도록 한다.</li>
  <li>시작할 때 7*7 CONV layer 를 이용한다.</li>
  <li>마지막 CONV layer 뒤에 global average pooling layer 로 pooling 을 진행한다.</li>
  <li>FC layer 는 output class 만을 위해 있고 더는 없다(no multiple FC layer).</li>
</ul>

<p>이런 구조를 가진 <em>ResNet</em> 은 34, 50, 101, 152 의 레이어를 갖는다.</p>

<p>50보다 깊은 레이어를 가진 <em>ResNet</em> 은 <em>GoogLeNet</em> 처럼 bottleneck layer 를 이용한다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image24.png" alt="residual block with bottleneck" /></p>

<p>이렇게 앞뒤로 1 by 1 CONV layer 를 이용해 입/출력의 depth 는 유지하면서 계산에서는 효율적이게 만들었다.</p>

<p>실제 <em>ResNet</em> 의 학습은 다음의 설정으로 이루어졌다.</p>

<ul>
  <li>매 CONV layer 가 진행되면 batch normalization</li>
  <li>Xavier /2 initialization 을 이용</li>
  <li>SGD + Momentum (momentum = 0.9) 이용</li>
  <li>learning rate = 0.1, valid error 가 평탄함을 보일때 마다 10만큼 나눠주며 decay</li>
  <li>미니 배치의 크기는 256</li>
  <li>weight decay = 1e-5</li>
  <li>dropout 은 사용하지 않음</li>
</ul>

<p>이렇게 사용된 <em>ResNet</em> 은 ImageNet 에서는 152개의 레이어, CIFAR 에서는 무려 1202개(!)의 레이어를 사용해도 성능의 저하 없이 학습이 가능했다.<br />
<em>ResNet</em> 으로 인해 매우 깊은 네트워크에서도 작은 training error 를 얻을 수 있게 되었다.</p>

<p>그리고 ILSVRC’15 의 모든 분야에서 압도적인 성능을 나타내며 1위를 차지하였다.<br />
classification 분야에서 3.6%의 top5 error 를 보였는데, 이는 인간의 능력(5%)를 상회하는 결과이다.</p>

<p>각 신경망들의 complexity 를 비교한 표를 보고 가자.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image25.png" alt="comparing NN's complexity" /></p>

<ul>
  <li>가장 높은 성능을 보이는 Inception-V4 는 사실 ResNet + Inception 이다.</li>
  <li>VGG는 많은 메모리와 연산이 필요한 것을 확인할 수 있다.</li>
  <li>GoogLeNet 이 굉장히 효율적이다.</li>
  <li>AlexNet 은 연산량이 작음에도 불구하고 메모리를 많이 필요로 하며 성능 역시 높지 않다.</li>
  <li>ResNet 은 모델에 따라 효율이 조금씩 다르지만 아주 높은 성능을 보인다.</li>
</ul>

<h4 id="deep-feature-fusion">Deep Feature Fusion</h4>

<p>이렇게 아주 좋은 성능을 내는 <em>ResNet</em> 이 발표되고 성능을 더 높이고자 하는 시도들이 이루어졌다.<br />
ILSVRC’16 에 classification 에서 1위를 차지한 <em>Fusion</em> 은 Inception, Inception-ResNet, RestNet, Wide ResNet 모델을 multi-scale 앙상블을 통해 성능을 높인 것이다.</p>

<h4 id="senet">SENet</h4>

<p><em>SENet(Squeeze-and-Excitation Networks)</em> 는 adaptive feature map reweighting 을 이용한 모델이다.<br />
‘feature recalibration’ 을 추가해 feature map 에 가중치를 적응형으로 다시끔 부여하는 학습이 가능하게 했다.<br />
global average pooling layer 와 두 개의 FC layer 를 사용해서 feature map 의 가중치를 결정했다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image26.png" alt="SEnet module" /></p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image27.png" alt="senet architecture" /></p>

<p>위와 같은 구조로 ILSVRC’17 에서 classification 분야 1위를 하게 되었다.<br />
이 때 ResNeXt-152 를 base architecture 로 사용했다.</p>

<p>2017년을 기점으로 ImageNet competition 은 열리지 않게 되고 Kaggle 로 넘어가게 되었다.</p>

<hr />

<h4 id="network-in-networknin">Network in Network(NiN)</h4>

<p>잠깐 몇 가지 추가적인 설명을 하고 넘어가자.</p>

<p>각 Conv layer 내에 ‘micronetwork’ 가 있는 MLPConv layer 는 로컬 패치(local patch)에 대한 추상적인 특징(abstract feature)을 계산해내는데 더 좋다.<br />
이 ‘micronetwork’ 는 MLP(Multilayered Perceptron)에 사용되었으며 이 개념이 GoogLeNet 과 ResNet 에 사용된 bottleneck layer 의 선구자이다.</p>

<p>local network topology 에 영향을 미친 개념이 바로 NiN 이 되겠다.</p>

<h4 id="identity-mappings-in-deep-residual-networks">Identity Mappings in Deep Residual Networks</h4>

<p>identity mapping 을 이용해 <em>ResNet</em> 의 block 에 대한 설계를 향상 시킬 수 있었으며,<br />
정보를 직접적으로 전파하는 방식을 이용해 더 좋은 성능을 내게 해준 것으로 자세한 것은 <a href="https://arxiv.org/abs/1603.05027">논문</a>을 확인해보자.</p>

<blockquote>
  <p>위에서 잠깐 언급한 identity mapping 이란 무엇일까?<br />
ResNet 의 residual block 에서 언급했던 $H(x)$ 를 identity mapping 을 위한 shortcut connection 이라 했지만 실제적으로 이는 identity 함수(f(x)=x 라던가) 와 residual 함수의 합에 ReLU 를 적용시킨 것이라 완전한 identity mapping 이라고 보기에 어렵다.<br />
논문에서는 f 가 identity 함수라면 어떨까에 대한 접근을 보여주고 있다. <small>언젠가 자세히 리뷰할 날을 꿈꾸며</small></p>
</blockquote>

<h4 id="wide-residual-networks">Wide Residual Networks</h4>

<p><em>ResNet</em> 을 향상시킨 또하나의 방법인 Wide Residual Network 이다.<br />
2016년 Zagoruyko 가 발표한 <a href="https://arxiv.org/abs/1605.07146">논문</a>에서 자세히 확인할 수 있지만 간단히 정리하면 다음과 같다.</p>

<ul>
  <li>depth 가 아니라 residual 이 중요한 것이라 주장</li>
  <li>F 개의 필터에 대해 k factor 를 곱해준 만큼의 필터를 하나의 block 으로 이용</li>
  <li>그 결과 50-layer wide ResNet 이 152-layer ResNet 보다 좋은 성능을 보임</li>
  <li>depth 를 높이는 것 보다 width 를 늘리는 것이 계산 상 효율적임을 보임(parallelizable)</li>
</ul>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image28.png" alt="wide residual netrowk" /></p>

<blockquote>
  <p>컴퓨터는 dense 한 연산에 대해 더 강한 모습을 보이는데, 이는 dropout 을 통해 deep 하지만 sparse 한 모델보다 dense 한 모델이 더 성능이 좋다는 것과 같은 결을 가진다.</p>
</blockquote>

<h4 id="resnext">ResNeXt</h4>

<p>ResNet 으로부터 탄생한 또 하나의 모델인 ResNeXt 는 여러개의 평행한 pathway(cardinality)를 이용해 width 를 늘리는 방식을 이용한다.<br />
이 평행한 pathway 는 Inception module 의 그것과 유사하다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image29.png" alt="resnext" /></p>

<h3 id="fractalnet">FractalNet</h3>

<p>이처럼 ResNet 을 이용해 성능의 향상을 도모하는 연구들이 많이 있으나, ResNet 과는 다른 구조이면서도 좋은 성능을 내는 것들도 연구되었다.</p>

<p><em>FractalNet</em> 이 그 중 하나인데, 이름에서도 알 수 있듯이 fractal 구조로 네트워크가 구성되어 있다.<br />
여기서는 residual representation 이 필요하지 않으며, 전달을 잘 하는 것이 중요하다고 주장한다.</p>

<p>training 시에 sub-path 들을 dropout 하는 식으로 학습시키며 테스트에서는 full network 를 사용한다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image30.png" alt="fractalnet" /></p>

<h3 id="densely-connected-convolutional-networks">Densely Connected Convolutional Networks</h3>

<p>앞 쪽 레이어들에 대해 각기 연결되는 dense block 을 이용하는 것이 핵심이다.<br />
이러한 구조를 갖게 되면 다음의 이점을 얻는다.</p>

<ul>
  <li>vanishing gradient 를 약화</li>
  <li>feature 의 전파를 강화</li>
  <li>feature 의 재사용을 장려하게 됨(계속 보내니까)</li>
</ul>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image31.png" alt="densely connected conv net" /></p>

<h3 id="mobilenets">MobileNets</h3>

<p>신경망은 기본적으로 많은 연산량과 파라미터를 필요로 한다.<br />
이는 모바일 같은 리소스의 제약이 존재하는 기기에서는 사용하는데 어려움을 주는 요소이다.</p>

<p>연구자들은 조금 더 가볍고 효율적인 신경망에 대해 연구하였고, <em>MobileNet</em> 은 그 연구 성과 중 하나이다.</p>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image33.png" alt="mobilenet" /></p>

<p>이는 다음과 같은 특징을 가진다.</p>

<ul>
  <li>standard convolution 을 대체하는 depthwise separable convolution 을 이용한다. 그림의 $D_k \times D_k \times 1$ 이 그것이다.</li>
  <li>loss 의 손실이 적으면서 많이 효율적이다.</li>
  <li>MobileNetV2(Sandler, 2018) 에 의해 더욱 개량되었다.</li>
  <li>ShuffleNet(Zhang 2017) 이라는 다른 연구도 있다.</li>
</ul>

<blockquote>
  <p>모바일 환경이 대세로 자리잡은 요즘, 이와 관련하여 더 공부해보도록 하자!<br />
standard Conv 은 채널당 N 개의 $D_k \times D_k \times M$ 의 필터를 적용했는데, depthwise Conv 에서는 각 채널당 1 개의 $D_k \times D_k \times 1$ 을 적용하게 된다.<br />
이를 통해 얻는 효과는 다음과 같다.</p>

  <blockquote>
    <p>파라미터 수의 변화 :<br />
  standard=$M \times D_k \times D_k \times N$<br />
  depthwise=$D_k \times D_k \times M + M \times N$</p>
  </blockquote>

  <p>과정을 간략히 표현하면 다음과 같다.<br />
각 채널에 $D_k \times D_k \times 1$ 필터를 적용해 중간 단계의 $H_1 \times W_1 \times M$ 을 얻는다. 그리고 여기서 1 by 1 conv 를 적용해 linear combination 을 수행해주면 $H_2 \times W_2 \times N$ 이 나오게 된다.<br />
stride 를 조절하면 $H_1 = H_2, W_1 = W_2$ 가 될 수도 있고 다를 수도 있다는 것을 참고하라.</p>

  <p><a href="https://arxiv.org/abs/1904.03775">reference</a></p>
</blockquote>

<h3 id="meta-learning">Meta-learning</h3>

<p>메타 학습 <em>(Meta-learning)</em> 은 learn network architecture 를 학습을 하는 것이다. 즉, 네트워크의 구조 자체를 학습하는 것이라 볼 수 있다.</p>

<p>2016년 Zoph 의 <a href="https://arxiv.org/abs/1611.01578">논문</a> Neural Architecture Search with Reinforecement Learning(NAS) 에 다음의 내용이 소개된다.</p>

<ul>
  <li>‘controller’ 네트워크는 좋은 네트워크 구조를 학습한다. 즉, 출력값이 네트워크의 디자인이 된다.</li>
  <li>다음의 과정을 반복한다.
    <ol>
      <li>search 공간에서 샘플 구조를 추린다.</li>
      <li>정확도에 따른 ‘보상’ R 을 얻기 위한 구조를 학습한다.</li>
      <li>샘플 확률에 대한 gradient 를 계산하고, R 로 scale 하여 controller parameter 업데이트를 진행한다. 즉 좋은 구조의 확률(likelihood)은 증가시키고, 나쁜 구조의 확률은 감소시킨다.</li>
    </ol>
  </li>
</ul>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image34.png" alt="meta learning nas" /></p>

<p>Zoph 는 이듬해 <a href="https://arxiv.org/abs/1707.07012">논문</a> Learning Transferable Architectures for Scalable Image Recognition 에서 그 후속 연구를 진행했다.</p>

<ul>
  <li><em>NAS</em> 에 ImageNet 같은 거대한 데이터셋을 적용하는 것은 비싼 작업이다.</li>
  <li>block <em>(cells)</em> 을 쌓는 것은 유연하게 이루어 질 수 있다.</li>
  <li><em>NASNet</em> 은 <em>NAS</em> 를 이용해 가장 좋은 cell structure 를 CIFAR-10 데이터셋과 유사한 곳에서 찾고 이 구조를 ImageNet 에 전이한다.</li>
  <li>이와 관련해 AmoebaNet, ENAS 등의 후속 연구가 진행되고 있다.</li>
</ul>

<p><img src="/assets/images/2019-10-14---cs231n-cnn-architectures/image35.png" alt="meta learning nasnet" /></p>

<hr />

<p>이번 강의를 요역하면 다음과 같다.</p>

<p>우리는 여기까지 CNN 의 다양한 구조 <em>(AlexNet, VGG, GoogLeNet, …)</em> 에 대해 학습했다.</p>

<p>실험적으로 <em>ResNet</em> 과 <em>SENet</em> 이 좋은 성능을 나타낸다.<br />
네트워크는 계속해서 깊어지고 있으며 네트워크의 구조 역시 계속 연구되며 향상되는 중이다.<br />
그리고 이제는 네트워크 구조 자체를 학습하는 <strong>meta-learning</strong> 의 연구로 나아가고도 있다.</p>

<blockquote>
  <p>이 포스트는 스탠포드의 <a href="http://cs231n.stanford.edu">cs231n</a> 9강 강의를 보고 공부 및 정리한 포스트입니다.<br />
잘못된 것이 있을 수 있습니다.<br />
댓글로 알려주시면 감사합니다!</p>
</blockquote>


<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">11</span>
      </a>
    </li>
  
    <li>
      <a href="#2019">
        <strong>2019</strong> <span class="taxonomy__count">33</span>
      </a>
    </li>
  
</ul>



  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-04-01---postgresql-tips/" rel="permalink">PostgreSQL 간단 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">PostgreSQL 이란
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/docker/2020-03-27---docker-overview/" rel="permalink">Docker Overview
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Docker 란
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/docker/2020-03-27---docker-compose-overview/" rel="permalink">Docker Compose Overview
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Docker Compose
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-01-15---anaconda-tips/" rel="permalink">Anaconda Tips
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Anaconda 관련해서 자주 사용하고 필요한 것들을 정리해보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2020-01-13---pose-estimation-terms/" rel="permalink">Pose Estimation 관련 용어 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Pose Estimation 관련하여 자주 나오는 용어들에 대해 간략히 정리하고 틈틈히 상기할 수 있도록 해보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2020-01-13---boj-5373/" rel="permalink">[BOJ 5373] 큐빙
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">풀이
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2020-01-13---boj-17142/" rel="permalink">[BOJ 17142] 연구소3
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">문제 출처 : https://www.acmicpc.net/problem/17142
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2020-01-10---rmpe-retional-multi-person-pose-estimation/" rel="permalink">RMPE: Regional Multi-person Pose Estimation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">RMPE: Regional Multi-person Pose Estimation 라는 논문에 대해 간략히 학습하였고 이를 바탕으로 내용을 정리해보도록 하자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-01-10---linux-command-line/" rel="permalink">Linux Command Line 명령어 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">자주 쓰는 command 정리
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-01-09---vi-vim-command-line/" rel="permalink">VI VIM Command Line 명령어 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">저장 및 종료
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2020-01-08---cascade-feature-aggregation-for-human-pose-estimation/" rel="permalink">Cascade Feature Aggregation for Human Pose Estimation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Cascade Feature Aggregation for Human Pose Estimation
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2019" class="taxonomy__section">
    <h2 class="archive__subtitle">2019</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-12-14---graph-neural-networks-2/" rel="permalink">Graph Neural Networks 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Graph Pooling
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/computer%20network/2019-12-11---computer-network-overview-part-2/" rel="permalink">Computer Network Overview Part 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  24 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">지난 포스트에 이어서 컴퓨터 네트워크와 관련해 개략적으로 알아보자.
이번 포스트에서 다룰 것들은 다음과 같다.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-12-11---graph-neural-networks-1/" rel="permalink">Graph Neural Networks 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-12-14
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-09---digital-signature/" rel="permalink">Digital Signature
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">정보보호에서 Digital Signature 란 무엇인지 간략히 학습해보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-03---message-authentication-code/" rel="permalink">Message Authentication Code
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Message Authentication
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-12-02---cs231n-lec13-visualizing-and-understanding/" rel="permalink">[cs231n-lec13] Visualizing and Understanding
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  이 포스트는 스탠포드의 cs231n 13강 강의를 보고 공부 및 정리한 포스트입니다.
잘못된 것이 있을 수 있습니다.
댓글로 알려주시면 감사합니다!

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-02---cryptographic-hash-functions/" rel="permalink">Cryptographic Hash Functions
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Hash Functions
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-02---other-public-key-cryptosystems/" rel="permalink">Other Public-Key Cryptosystems
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일: 2019-12-02
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-01---public-key-cryptography-and-rsa/" rel="permalink">Public-Key Cryptography and RSA
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일: 2019-12-01
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-11-25---introduction-to-number-theory-for-information-security-2/" rel="permalink">Introduction to Number Theory for Information Security 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">지난 포스트 에 이어 암호학에 있어서 어떤 수학적인 내용들이 사용되는지 살펴보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-11-22---cs231n-lec11-generative-models/" rel="permalink">[cs231n-lec11] Generative Models
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-11-29
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-11-19---cs231n-lec12-detection-and-segmentation/" rel="permalink">[cs231n-lec12] Detection and Segmentation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-11-22
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-11-01---introduction-to-number-theory-for-information-security-1/" rel="permalink">Introduction to Number Theory for Information Security 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">정보보호와 암호학에 대해 공부를 하다보면 마주하게 되는 수학이 있다.
여기서 정수론(Number Theory)에 대해 간략하게 맛보고 가보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-11-01---cs231n-lec10-recurrent-neural-networks/" rel="permalink">[cs231n-lec10] Recurrent Neural Networks
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-11-19
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2019-10-30---netlify-migration/" rel="permalink">Netlify 로 github 블로그 이전하기
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Netlify
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2019-10-18---boj-12886/" rel="permalink">[BOJ 12886] 돌 그룹
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">문제 출처 : https://www.acmicpc.net/problem/12886
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/computer%20network/2019-10-16---computer-network-overview/" rel="permalink">Computer Network Overview Part 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  30 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-22
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-15---block-cipher-operation/" rel="permalink">Block Cipher Operation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">이전 포스트 에서 DES 에서 더 보안성이 강화된 AES 까지 알아보았다.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-14---cs231n-cnn-architectures/" rel="permalink">[cs231n-lec9] CNN Architectures
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  15 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-15
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-13---cs231n-traning-neural-networks-part-2/" rel="permalink">[cs231n-lec8] Training Neural Networks, Part 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  17 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-14
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-13---cs231n-training-neural-networks-part-1/" rel="permalink">[cs231n-lec7] Training Neural Networks, Part 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-13
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-12---cs231n-convolutional-neural-networks/" rel="permalink">[cs231n-lec5] Convolutional Neural Networks
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-12
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-12---advanced-encryption-standard/" rel="permalink">Advanced Encryption Standard
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">AES
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-11---cs231n-neural-networks-and-backpropagation/" rel="permalink">[cs231n-lec4] Neural Networks and Backpropagation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-11
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-11---block-ciphers-and-the-data-encryption-standard/" rel="permalink">Block Ciphers and the Data Encryption Standard
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  6 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Block Ciphers
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-11---cs231n-loss-functions-and-optimization/" rel="permalink">[cs231n-lec3] Loss Functions and Optimization
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-17
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-10---cs231n-image-classification-pipeline/" rel="permalink">[cs231n-lec2] Image Classification Pipeline
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-16
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-10---classical-encryption-techniques/" rel="permalink">Classical Encryption Techniques
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">고전적인 암호화 기법
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-09---information-security-overview/" rel="permalink">정보보호(Information Security) 공부를 시작하며
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">정보보호란?
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2019-10-09---boj-2125/" rel="permalink">[BOJ 2125] Mothy
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">[BOJ 2125] Mothy
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2019-10-04---markdown-usage/" rel="permalink">Markdown을 써보자
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Markdown 이란
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2019-10-02---boj-17503/" rel="permalink">[BOJ 17503] 맥주 축제
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">[BOJ 17503] 맥주 축제
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2019-10-01---vs-code-c++-configuration-for-mac/" rel="permalink">맥에서 VS Code C++ 빌드 설정하기
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">맥에서 VS Code로 C++ 빌드를 해보자
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>


  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Cheong. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/machine%20learning/2019-10-14---cs231n-cnn-architectures/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/machine%20learning/--cs231n-cnn-architectures"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://younghk-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
