<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.19.3 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[cs231n-lec8] Training Neural Networks, Part 2 - Cheong’s log</title>
<meta name="description" content="cs231n lec 8 강을 보고 정리한 글입니다.">


  <meta name="author" content="Younghk">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Cheong's log">
<meta property="og:title" content="[cs231n-lec8] Training Neural Networks, Part 2">
<meta property="og:url" content="http://localhost:4000/machine%20learning/2019-10-13---cs231n-traning-neural-networks-part-2/">


  <meta property="og:description" content="cs231n lec 8 강을 보고 정리한 글입니다.">







  <meta property="article:published_time" content="2019-10-13T02:51:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/machine%20learning/2019-10-13---cs231n-traning-neural-networks-part-2/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Cheong",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Cheong's log Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--post">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/profile.png" alt=""></a>
        
        <a class="site-title" href="/">
          Cheong's log
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tags</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Younghk</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>오직 부끄러워야 할 것은 열심히 살지 않은 어제의 나이다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">Seoul, KR</span>
        </li>
      

      
        
          
        
          
            <li><a href="https://younghk.github.io" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i><span class="label">Website</span></a></li>
          
        
          
        
          
        
          
            <li><a href="https://github.com/younghk/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
        
          
            <li><a href="https://www.linkedin.com/in/younghoon-kim-03a667191/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <div class="archive">
    
      <h1 id="page-title" class="page__title">[cs231n-lec8] Training Neural Networks, Part 2</h1>
    
    <p><small>최종 수정일 : 2019-10-14</small></p>

<p>지난 시간, 우리는 활성 함수를 비롯해 초기화, 정규화 등에 대해서 학습하였다.</p>

<p><em>Neural Network</em> 이려면 적당한 <em>activation function</em> 이 필요하며 이것의 부재는 결국 linear function 이 될 뿐이다.</p>

<p>계속해서 <em>Neural Net</em> 을 학습하는데 필요한 것들을 살펴보자.</p>

<h2 id="optimization">Optimization</h2>

<p>이제 우리는 최적화(<em>Optimization</em>) 알고리즘에 대해 살펴볼 것이다.<br />
무엇을 최적화하는 것일까? 답을 찾아가는 과정이라고 생각하면 편할 것 같다.</p>

<h3 id="sgd">SGD</h3>

<p>이미 우리는 간단하게 <em>Stochastic Gradient Descent</em> 를 학습했다.<br />
<em>SGD</em> 는 <em>full GD(gradient descent)</em> 를 하기에 힘든 경우, 하나만 보아서 그 속도를 높이는 방법이었다.<br />
이는 코드로 작성하기도 굉장히 쉽다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Vanilla Gradient Descent
</span><span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">weights_grad</span> <span class="o">=</span> <span class="n">evaluate_gradient</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">weigths</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">+=</span> <span class="o">-</span> <span class="n">step_size</span> <span class="o">*</span> <span class="n">weights_grad</span> <span class="c1"># perform parameter update
</span></code></pre></div></div>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image1.png" alt="vanilla gd" /></p>

<p>이러한 <em>SGD</em> 로는 다음과 같은 하얀색 방향을 찾을 수 있고 우리는 여기서 가장 붉은 부분을 찾아 내려가고 싶은 것이다. 그림의 파란선처럼 따라가게 되면 적당할 것으로 보인다.</p>

<p>그러나 실제로 <em>SGD</em> 가 저렇게 부드럽게 답을 찾아가는 것은 어렵다.<br />
다음의 예를 생각해보자.<br />
좌우로는 gradient 의 변화가 적으나 위아래로는 gradient 의 변화가 크다면? 즉, 한쪽 방향으로만 굉장히 sensitive 한 경우 어떻게 될 것인가?</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image2.png" alt="sgd problem" /></p>

<p>이와 같은 과정을 거치며 optimizer 로 향할 것이다. 이는 <em>full GD</em> 가 아니기에 충분히 일어날 수 있는 일이다.</p>

<blockquote>
  <p>수학적으로, Hessian 의 condition number 가 크다는 것을 의미한다. 이는 poor conditioned matrix 가 되는데, 간단하게 생각했을 때 좌우로 굉장히 긴 ellipsoid 가 이러한 Hessian matrix 를 가지게 된다.</p>
</blockquote>

<p>저런 식의 진행은 굉장히 오랜 시간이 걸리는 작업이 된다. -&gt; 좋지 못함!</p>

<p>또는 <strong>local minima</strong> 와 <strong>saddle point</strong> 에서도 문제점을 확인할 수 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image3.png" alt="local minima and saddle point" /></p>

<p>위와 같은 상태인데, 각각 <em>gradient = 0</em> 이 되기 때문에 멈춰버리게된다.<br />
<em>saddle point</em> 가 아니더라도 0에 굉장히 가깝다면 매우 느려지는 문제도 있다.<br />
이는 사실 <em>SGD</em> 가 아닌 <em>GD</em> 에서도 충분히 일어날 수 있는 문제이다.<br />
예시의 2차원이 아닌 다차원 공간에 이러한 지점이 얼마나 많을지 생각해보라!</p>

<p>그리고 <em>SGD</em> 는 그 방향이 최적화된 방향이 아니기에 실은 아래와 같이 움직이는게 더 일반적이다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image4.png" alt="sgd moving" /></p>

<h3 id="momentum">Momentum</h3>

<p>이러한 문제는 사실 간단하게 해결할 수 있는데, 바로 <strong>관성(Momentum)</strong>을 이용하는 것이다!<br />
<em>SGD</em> : $x_{t+1} = x_t - \alpha \nabla f(x_t)$<br />
에서 <em>속도(velocity)</em> 를 이용한 관성을 추가해주면 다음과 같다.<br />
<em>SGD + Momentum</em> :</p>

\[\begin{aligned}
v_{t+1} &amp;= \rho v_t + \nabla f(x_t) \\
x_{t+1} &amp;= x_t - \alpha v_{t+1}
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">vx</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">vx</span> <span class="o">+</span> <span class="n">dx</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">vx</span>
</code></pre></div></div>

<p>여기서 $\rho$ 가 <em>friction</em> 으로 사용되게 되는데, 0.9 또는 0.99로 보통 값을 설정한다.</p>

<p>이를 적용하면 조금은 부드러운 움직임을 보이게 된다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image5.png" alt="sgd momentum" /></p>

<h3 id="nesterov-momentum">Nesterov Momentum</h3>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image6.png" alt="momentum update" /></p>

<p>이렇게 관성을 이용해 actual step 을 정하는 방법에서 더 나아가 <em>Nesterov Momentum</em> 은 다음과 같이 다음 스텝을 정한다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image7.png" alt="nesterov momentum" /></p>

<p>이 <em>look ahead</em> 라 불리는 방법은 관성을 줬을 때의 지점(point) 에서의 <em>gradient</em> 를 계산해 이를 다음 step 으로 설정하는데 이용하는 것이다.</p>

<p>이는 current point 에서 <em>gradient</em> 를 이용해 다음으로 나아가는 방식을 깬 기법이다.<br />
수학적으로는 아래와 같이 표현할 수 있다.</p>

\[\begin{aligned}
v_{t+1} &amp;= \rho v_t - \alpha \nabla f(x_t + \rho v_t) \\
x_{t+1} &amp;= x_t + v_{t+1}
\end{aligned}\]

<p>그러나 우리는 current point 에서의 계산을 선호하므로 식을 조금 변형($\tilde{x}_t = x_t + \rho v_t$)하면 다음과 같이 쓸 수 있다.</p>

\[\begin{aligned}
v_{t+1} &amp;= \rho v_t - \alpha \nabla f(\tilde{x}) \\

\tilde{x}_{t+1} &amp;= \tilde{x}_t - \rho v_t + (1 + \rho) v_{t+1} \\
&amp;= \tilde{x}_t + v_{t+1} + \rho(v_{t+1} - v_t)
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">old_v</span> <span class="o">=</span> <span class="n">v</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">rho</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">rho</span> <span class="o">*</span> <span class="n">old_v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">rho</span><span class="p">)</span> <span class="o">*</span> <span class="n">v</span>
</code></pre></div></div>

<p>이는 <em>SGD + Momentum</em> 보다 더 부드러운 곡선을 그리며 진행하게 된다.</p>

<blockquote>
  <p>굉장히 sharp 한 minima 같은 것이 존재해 거기에 빠져버리게 된다면 어떡하는가?<br />
물론 그럴 수도 있으나 data 를 충분히 모으게 되면 그런 sharp 한 minima 는 사라질 수 있다. 이러한 경우 이는 noise 인 셈이므로 bad minima 이다.</p>
</blockquote>

<h3 id="adagrad">AdaGrad</h3>

<p>이렇게 관성을 이용하면 <em>local minima</em> 와 <em>saddle point</em> 를 빠져나올 수 있고 부드럽게 움직이는게 가능하다. 이를 가능하게 하는 이유는 momentum 은 느린 부분이 모이면 가속되게 하고, 빠르게 진동하는 부분은 상쇄하는 효과를 가지기 때문이다.</p>

<p>그렇다면 계속 가속되는 경우는 어떻게 대처해야 할까?</p>

<p>그래서 우리는 관성 대신 <code class="language-plaintext highlighter-rouge">grad_squared</code> 라는 개념을 도입했다.<br />
이는 <em>adaptive learning rate</em> 라고도 불리운다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gard_squared</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">grad_squared</span> <span class="o">+=</span> <span class="n">dx</span> <span class="o">*</span> <span class="n">dx</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">lerarning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">grad_sqaured</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
</code></pre></div></div>

<p>이 <code class="language-plaintext highlighter-rouge">grad_squared</code> 를 도입하게 되면 무슨 변화가 있는 것일까?<br />
이는 진행 과정에 있어서 가파른 구간 <em>(steep)</em> 에서는 상쇄하는 효과를 가져오며, 평평한 구간 <em>(flat)</em> 에서는 가속하는 효과를 가져온다!</p>

<p>그러나 시간이 지남에 따라 0으로 작아지게 되는 문제점이 있어서 <em>saddle point</em> 나 그러한 구간이 길어지면 진행이 멈춰버리게 된다.</p>

<p>여기서 <code class="language-plaintext highlighter-rouge">1e-7</code> 을 더하는 이유는 0으로 나누어지는 경우를 방지하기 위함이다. 검증된 수치는 아니나 충분히 작은 숫자이기에 현실적으로 잘 작동한다.</p>

<h3 id="rmsprop">RMSProp</h3>

<p><em>RMSProp</em> 은 <em>Leaky AdaGrad</em> 이다.<br />
<code class="language-plaintext highlighter-rouge">grad_squared</code> 부분에 <code class="language-plaintext highlighter-rouge">decay_rate</code> 라는 개념을 도입했다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gard_squared</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">grad_squared</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">grad_squared</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">*</span> <span class="n">dx</span>
    <span class="n">x</span> <span class="o">-=</span> <span class="n">lerarning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">grad_sqaured</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
</code></pre></div></div>

<p>여기서 <code class="language-plaintext highlighter-rouge">decay_rate</code> = 0.9 또는 0.99 가 보편적이다.</p>

<p>이를 통해 조금 더 최적화한 움직임을 확인할 수 있다.</p>

<h3 id="adam">Adam</h3>

<p>우리는 관성과 <em>decay rate</em> 에 대해 알아보았다.<br />
두 개를 같이 쓰면 더 좋은 결과가 있지 않을까?</p>

<p>다음은 관성과 <em>decay rate</em> 를 사용하는 Adam 의 코드이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_moment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">second_moment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">first_moment</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">first_moment</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>              <span class="c1"># Momentum
</span>    <span class="n">second_moment</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">second_moment</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">*</span> <span class="n">dx</span>       <span class="c1"># AdaGrad / RMSProp
</span>    <span class="n">x</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">first_moment</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">,</span><span class="n">sqrt</span><span class="p">(</span><span class="n">second_moment</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span> <span class="c1"># AdaGrad / RMSProp
</span></code></pre></div></div>

<p>첫 번째 step 에서 어떤 일이 있을까?<br />
second_moment 가 0에 가까울 것이다. 왜냐하면 어차피 0에 가까운 수를 곱하는 결과 이기 때문이다.<br />
이거를 해결하기 위해서는 아주 큰 step 을 이용해야하는데 이는 초기화를 굉장히 생각할 수 없는 방향으로 하는 것이라 좋은 결과를 가져올 수도 있으나 보통 이상한데로 가버려서 돌아오지 못하게 될 수 있다.</p>

<p><strong>Adam</strong> 은 위와 같이 관성과 <em>decay rate</em> 와 더불어 <em>bias correction</em> 도 해주게 된다. 따라서 완전한 코드는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">first_moment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">second_moment</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">first_moment</span> <span class="o">=</span> <span class="n">beta1</span> <span class="o">*</span> <span class="n">first_moment</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>
    <span class="n">second_moment</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">*</span> <span class="n">second_moment</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">*</span> <span class="n">dx</span>
    <span class="n">first_unbias</span> <span class="o">=</span> <span class="n">first_moment</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># bias correction
</span>    <span class="n">second_unbias</span> <span class="o">=</span> <span class="n">second_moment</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">t</span><span class="p">)</span> <span class="c1"># bias correction
</span>    <span class="n">x</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">first_unbias</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">second_unbias</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
</code></pre></div></div>

<p>이러한 <em>bias correction</em> 을 하는 이유는 first 와 second moment 가 0 에서 시작한다는 가정 때문이다.</p>

<p>beta1 = 0.9, beta2 = 0.999, learning<em>rate = 1e-3 또는 5e-4 의 _hyperparameter</em> 를 이용하는 <em>Adam</em> 은 굉장히 좋은 알고리즘이다. 대부분의 문제에 대해서 괜찮은 성능을 나타내는 것으로 알려져 있다.<br />
따라서 위의 설정으로 해본 후 다른 알고리즘을 try out 해보자.</p>

<hr />

<p>지금까지 배운 모든 알고리즘은 <strong>learning rate</strong> 를 <em>hyperparameter</em> 로 가지고 있다. 그렇다면 어떤 learning rate 가 가장 적당할까?</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image8.png" alt="learning rate" /></p>

<p>사실 다 괜찮다! 처음에 큰 learning rate 로 시작했다가 서서히 decay 시켜가며 확인해보는 것이 좋은 진행 과정이 될 것이다.</p>

<h3 id="learning-rate-decay">Learning Rate Decay</h3>

<p>learning rate decay 는 다양한 방법이 있을 수 있다. 진행 과정에서 learning rate 를 낮춰보는 다양한 방법을 살펴보자.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image9.png" alt="resnet learning rate decay" /></p>

<p>위의 방법은 0.1 만큼 곱해서 <em>learning rate</em> 를 줄이는 <em>ResNet</em> 의 예제이다. 30, 60, 90 epoch 에서 줄이는 것을 확인할 수 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image10.png" alt="cosine learning rate decay" /></p>

<p>다음과 같이 코사인 함수를 이용한 decay 방법도 존재한다.</p>

<p>$\alpha_t = {1 \over 2}\alpha_0 \left( 1 + \cos\left(t\pi / T \right) \right)$</p>

<p>이를 적용하여 loss 보면 다음과 같다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image11.png" alt="cosine learning rate decay result" /></p>

<p>선형 함수로도 learning rate 를 decay 해 볼 수 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image12.png" alt="linear learning rate decay" /></p>

<p>$\alpha_t = \alpha_0(1 - t / T)$ 로 표현되는 방식이다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image13.png" alt="inverse sqrt learning rate decay" /></p>

<p>$\alpha_t = \alpha_0 / \sqrt{t}$ 인 inverse sqrt 를 이용하는 방식도 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image14.png" alt="linear warmup learning rate decay" /></p>

<p>초기에 learning rate 가 높으면 loss explode 를 일으킬 수 있기에 첫 5000번의 iteration 에서 linear 하게 learning rate 를 증가시킨 다음 서서히 낮추면 그러한 문제점을 방지할 수 있다.</p>

<blockquote>
  <p>이처럼 다양한 learning rate decay 를 수행할 수 있다. 이 부분은 상당히 최근의 연구된 것들이라 관련 논문들에 대해 조금 더 찾아볼 필요가 있다.</p>
</blockquote>

<h3 id="first--second-order-optimization">First &amp; Second-Order Optimization</h3>

<p>지금까지 알아본 최적화 알고리즘들은 모두 <em>first-order Optimization</em> 이다.<br />
이는 <em>linear approximation(선형 근사)</em> 을 이용해 다음 스텝으로 나아가는 방식이다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image15.png" alt="linear approximation" /></p>

<p>이처럼 current point 에서 접선의 방정식을 구해 <em>gradient</em> 를 얻고, approximation 이 최소가 되도록 스텝을 내딛는 것이다.<br />
그림을 보아도 그 갭이 꽤 있다는 사실을 금방 확인할 수 있는데, 그렇다면 <em>second-order Optimization</em> 은 어떨까?</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image16.png" alt="second order optimization" /></p>

<p><em>second-order optimization(이차 근사)</em> 는 approximation 을 일차 함수가 아닌 이차 함수로 하는 것이다.<br />
이는 gradient 와 <strong>Hessian</strong> 을 사용해 <em>quadratic approximation</em> 을 하고 최소값의 지점을 다음 스텝으로 하게 된다.<br />
이 때, 일차 근사와는 다르게 step size 가 없다는 사실을 확인할 수 있는데 이차 근사에서는 <strong><em>항상 최소값</em></strong> 으로 가면 된다.</p>

<p>이는 Tayolr expansion 을 이용해 수식으로 표현할 수 있다.</p>

\[J(\theta) \approx J(\theta_0) + \left( \theta - \theta_0 \right)^T \nabla_\theta J(\theta_0) + {1 \over 2}(\theta - \theta_0)^TH(\theta - \theta_0)\]

<p>이를 정리해서 풀어내면 <em>Hessian</em> 의 역행렬로 다음 스텝을 구할 수 있게 된다.</p>

\[\theta^* = \theta_0 - H^{-1}\nabla_\theta J(\theta_0)\]

<p>이러한 <em>second-order Optimization</em> 은 수렴 속도가 상당히 빠른데 deep learning 에서 이러한 테크닉은 잘 쓰이기 어렵다.<br />
<em>Hessian</em> 은 $O(N^2)$의 시간 복잡도를 가지고 그 역행렬은 $O(N^3)$ 이기 때문에 N 이 큰 deep learning 에서는 너무나도 계산이 오래 걸리게 되기 때문이다.</p>

<p>이를 보완하기 위해 <em>Quasi-Newton methods(<strong>BFGS</strong>)</em> 라던가, <strong><em>L-BFGS</em></strong>(Limited memory BFGS) 등도 있으나 여전히 큰 메모리이거나 메모리에 저장이 안되어서 힘들다.</p>

<p><em>L-BFGS</em> 의 경우 full batch 에서는 잘 작동하기는 하나 mini-batch 에서는 아직 연구 중에 있다.</p>

<p><small>Convex Optimization 에서 조금 더 자세히 다뤄보자…</small></p>

<p>현실적으로 우리는 <strong><em>Adam</em></strong> 이 좋은 선택지가 되며, <em>SGD + Momentum</em> 도 learning rate 와 schedule 을 잘 설정하면 더 좋은 결과를 얻을 수 있을 것이다.</p>

<hr />

<p>우리는 지금까지 최적화 알고리즘과 그 동작에 대해 배웠고, 이를 통해 loss 가 줄어드는 방향을 어떻게 찾아가는지 학습했다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image17.png" alt="training error" /></p>

<p>그러나 실제로 test 시에 정확도를 높이기 위해서는 어떤 방법을 사용해야할까?<br />
training set 에 대한 정확도 보다는 <em>unseen data</em> 에서 좋은 성능을 원하는 우리가 할 수 있는 방법은 무엇이 있을까?</p>

<p>최적화 알고리즘을 잘 설정한다면 loss 는 줄어드는 경향을 보일 것이다.<br />
그러나 정확도는 조금 다른 양상을 보일 수 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image18.png" alt="early stopping" /></p>

<p>이처럼 iteration(epoch)이 반복될 수록 test 시에 validation set 에 대한 성능이 낮아지는 경우가 있을 수 있다. 이러한 경우 learning 을 멈춰야 한다.(설령 training 에서 계속 올라가고 있다고 해도!)<br />
training 시간이 오래 걸리는 경우에도 항상 <em>model snapshot</em> 을 통해 <em>val</em> 에서 잘 작동하고 있는지 추적해줘야 한다.</p>

<h3 id="ensemble">Ensemble</h3>

<p>모델의 정확도를 높여 좀 더 높은 성능을 얻기 위해서 시도해 볼 수 있는 방법 중 아주 간단하면서도 신기한 방법이 있다.<br />
바로 앙상블(<em>Ensemble</em>)이다.<br />
이 기법은 각각의 모델들을 학습시킨 후 그 결과를 평균 내는 방법이다.<br />
따로 무엇인가를 해주지 않지만 보편적으로 2%의 성능향상을 가지고 온다.</p>

<p>이는 overfitting 을 줄이고 performance 를 향상시키는 것으로 해석이 가능한데, 비록 급격한 성능의 향상은 없어도 일정 부분 성능 향상을 이끌 수 있는 방법이다.</p>

<p>그러나 우리는 단일 모델에 대한 성능을 높이고도 싶을 것이다. 여기에도 앙상블을 이용해 성능의 향상을 가지고 올 수 있는데, 단일 모델에 대해 여러 <em>snapshot</em> 을 training 과정에서 사용하는 것이다.</p>

<p><a href="https://arxiv.org/abs/1704.00109">paper for snapshot ensemble</a> « 자세한 내용은 논문을 확인하자!</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image30.png" alt="snapshot ensemble" /></p>

<p>위 도식은 링크된 논문에서 소개된 snapshot ensemble 을 시각화한 것이다.<br />
이런 식으로 하나의 모델에 대한 앙상블을 적용해(+ 스케쥴) 성능 향상을 도모할 수도 있다.</p>

<h2 id="regularization제약">Regularization(제약)</h2>

<p>단일 모델의 성능을 조금 더 향상시킬 수 있는 방법이 무엇이 있을까? <small>여기서 성능을 높이는 것은 <em>unseen data</em> 에 대한 성능을 높이는 것이다.</small><br />
그에 대한 답변으로 <strong>Regularization</strong> 기법이 있다.</p>

<p><strong>regularization</strong> 은 이전에도 살펴보았듯이 loss 를 얻기 위해 term 을 추가하는 것이다.<br />
간략하게 기억을 되살려보면,</p>

\[L = {1 \over N} \sum_{i=1}^N \sum_{j\ne y_i} max\left(0, f\left(x_i;W \right)_j - f\left(x_i;W \right)y_i + 1\right) + \lambda R\left( W \right)\]

<p>에서 $\lambda R(W)$ 가 <em>regularization</em> 이었고 보통 <strong>L2 regularization</strong> 을 많이 쓴다.</p>

<p>어떤 제약을 주는지 살펴보자.</p>

<h3 id="dropout">Dropout</h3>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image19.png" alt="dropout" /></p>

<p>forward pass 에서 임의의 뉴런을 꺼버리는 것이 <strong>dropout</strong> 이다.<br />
굉장히 단순한데 뉴런을 꺼버리는 확률을 <em>zero Probability</em> 라고 하며 <strong>0.5</strong> 를 많이 사용한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># probability of keeping a unit active. higher = less dropout
</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="s">""" X contains the data """</span>

    <span class="c1"># forward pass for example 3-layer neural network
</span>    <span class="n">H1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">H1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span> <span class="c1"># first dropout mask
</span>    <span class="n">H1</span> <span class="o">*=</span> <span class="n">U1</span> <span class="c1"># drop!
</span>    <span class="n">H2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">H1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">U2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">H2</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span> <span class="c1"># second dropout mask
</span>    <span class="n">H2</span> <span class="o">*=</span> <span class="n">U2</span> <span class="c1"># drop!
</span>    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">H2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>

    <span class="c1"># backward pass: compute gradients... (not shown)
</span>    <span class="c1"># perform parameter update... (not shown)
</span></code></pre></div></div>

<p>이와 같이 매우 간결한 코드로 수행이 가능하다.<br />
이 때, backward pass 에서 drop 된 뉴런도 되어야 한다.</p>

<p>문득 생각해보면 어떻게 이게 좋은 결과를 가지고 오게 되는지 이해가 안갈 수 있다.<br />
<em>Dropout</em> 은 network 에 강제로 중복된 표현(redundant representation) 을 부여하고, feature 들의 공동 적응(co-adaptation)을 방지해주는 효과를 가진다.</p>

<p>고양이 사진에 대한 학습을 예로 생각해 보자.<br />
귀와 꼬리에 대한 뉴런이 각각 있다가 귀에 대한 뉴런이 dropout 되면, 주로 꼬리를 보다가 귀도 보게끔 학습이 이루어 지는 식이다.<br />
이는 너무 특정한 feature 에 매몰되는 것을 막는 것인데, 모든 부분에 대한 학습을 시도할 경우 제대로 학습이 안될 수도 있고, noise 를 제거하는 효과로도 생각할 수 있다.</p>

<p>또 다른 관점은 <em>dropout</em> 과정이 모델에 대한 거대한 ensemble 과정이라는 것이다. 가중치를 공유하는 여러 모델들이라고 생각을 할 때, 여러 번의 dropout 으로 생성되는 모델들의 결과가 모여 더 좋은 성능을 보여준다는 의미이다.</p>

<p>그런데 이러한 <em>dropout</em> 은 output 도 랜덤하게 만들 수 있다.<br />
오늘 <em>고양이</em> 라고 판별한 사진이 내일 돌려보니 <em>나무</em> 라고 판별된다면 이 모델을 신뢰할 수 있을까?</p>

<p>우리는 그래서 test-time 에 있어서 <em>“average out”</em> 이 필요하다. 그리고 이는 수학적으로 표현하면 다음과 같이 할 수 있다.</p>

\[\begin{aligned}
y &amp;= f_W(x,z) \qquad \text{ y is output, z is random mask }\\
y &amp;= f(x) = E_z\left[ f(x,z) \right] = \int p(z)f(x,z)dz
\end{aligned}\]

<p>그러나 학습 과정에서 적분을 수행할 수 있을까…? 쉬울까…?</p>

<p>다행히 approximated 한 적분 결과값을 얻는 방법이 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image20.png" alt="single neuron" /></p>

<p>위와 같은 단일 뉴런을 생각하자.<br />
test-time 때 우리는 $E[a] = w_1x + w_2y$ 가 됨을 알 수 있는데,<br />
각각의 경우에 대해 모두 계산하면 다음과 같은 결과를 얻게 된다.</p>

<p>training time :</p>

\[\begin{aligned}
E[a] &amp;= w_1x + w_2y \\
&amp;= {1 \over 4}(w_1x + w_2y) + {1 \over 4}(w_1x - 0y) + {1 \over 4}(0x + 0y) + {1 \over 4}(0x + w_2y) \\
&amp;= {1\over 2}(w_1x + w_2y)
\end{aligned}\]

<p>즉, test-time 때 dropout probability 를 곱해주게 된다는 것이다.<br />
이 확률을 test-time 때 고정적으로 사용하게 되면 일정한 output 을 얻게 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># ensembled forward pass
</span>    <span class="n">H1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span> <span class="c1"># NOTE: scale the activations
</span>    <span class="n">H2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">H1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="o">*</span> <span class="n">p</span> <span class="c1"># NOTE: scale the activations
</span>    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">H2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
</code></pre></div></div>

<p>test-time 에는 모든 뉴런이 항상 active 되어야 한다. =&gt; scale the activations 를 해야 각 뉴런에 대해 <em>output at test time = expected output at training time</em> 이 된다.</p>

<p>여기서 우리는 test-time 때 scale 을 해주고 있게 되는데, 이를 train 때 미리 scaling 을 해줄 수도 있다!<br />
이는 test time 에서 변화가 없게(unchanged) 하는 방법인데, 보통 training 때 GPU 를 사용하기에 이 정도 연산이 추가되는 것은 큰 차이가 없으나, test-time 에 있어서는 더 효율적(efficiency)이게 될 수 있기 때문이다.</p>

<p>따라서 코드를 정리하면 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">p</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># probability of keeping a unit active. higher = less dropout
</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="s">""" X contains the data """</span>

    <span class="c1"># forward pass for example 3-layer neural network
</span>    <span class="n">H1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="n">U1</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">H1</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span> <span class="c1"># first dropout mask. Notice /p!
</span>    <span class="n">H1</span> <span class="o">*=</span> <span class="n">U1</span> <span class="c1"># drop!
</span>    <span class="n">H2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">H1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">U2</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">H2</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">)</span> <span class="o">/</span> <span class="n">p</span> <span class="c1"># second dropout mask. Notice /p!
</span>    <span class="n">H2</span> <span class="o">*=</span> <span class="n">U2</span> <span class="c1"># drop!
</span>    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">H2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>

    <span class="c1"># backward pass: compute gradients... (not shown)
</span>    <span class="c1"># perform parameter update... (not shown)
</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="c1"># ensembled forward pass
</span>    <span class="n">H1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span> <span class="c1"># no scaling neccesary
</span>    <span class="n">H2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">H1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">H2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
</code></pre></div></div>

<p>이와 같이 <em>regularization</em> 을 적용하는 것은 training 과 test 에서 차이가 조금 있는데, <em>Batch Normalization</em> 의 경우도 살펴보자.<br />
training 시에 랜덤 미니배치에 대해 통계적으로 정규화를 시키고 testing 시에 noramlised 된 값을 고정적으로 사용만 할 뿐이다.</p>

<h3 id="data-augmentation">Data Augmentation</h3>

<p><em>Regularization</em> 에서 또 다른 기법인 <em>data augmentation</em> 에 대해 알아보자.<br />
이 기법은 이미 가지고 있는 데이터셋(라벨링이 되어있음)에서 데이터들에 변형을 가해서 데이터셋을 늘리는 방식이다.<br />
이 때, 변형된 이미지는 이미 라벨링된 데이터이므로 손쉽게 dataset 을 늘릴 수 있게 된다.(지도학습에 있어서 유리)</p>

<p>즉 이미지를 transform(변형) 하게 되는데, 다음과 같은 변형 방식이 있다.</p>

<ul>
  <li>좌우 대칭(horizontal flip)<br />
 <img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image21.png" alt="horizontal flip" /></li>
  <li>
    <p>random crop and scale<br />
 <img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image22.png" alt="random crop and scale" /><br />
 ResNet 에서는 다음과 같이 적용한다.<br />
 Training : 랜덤 crop/scale 샘플을 만든다.</p>

    <ol>
      <li>Random L(256~480) 을 정한다.</li>
      <li>image 의 짧은 쪽 길이를 L 로 재조정한다.</li>
      <li>random 한 224*224 patch 를 샘플링한다.</li>
    </ol>

    <p>Test : crop 된 set의 평균을 낸다.</p>

    <ol>
      <li>이미지를 (224, 256, 384, 480, 640)의 크기를 갖게 한다.</li>
      <li>각 사이즈에 대해 10개의 224*224 crop 을 적용한다.(4 corner + center &amp; + flip)</li>
    </ol>
  </li>
  <li>Color Jitter
<img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image23.png" alt="color jitter" />
이미지에 랜덤한 색대비(contrast)나 밝기(brightness) 조정을 준다.<br />
 RGB에 PCA 를 이용하는 방법도 있으나 흔히 쓰이지는 않는다.</li>
  <li>그 외 여러 가지 방법(translation, rotation, strecthing, shearing, lens distortions, …)이 있을 수 있다.</li>
</ul>

<p>위에서 본 것들을 정리하면, 일반적으로 training 시에 random noise 를 부여하고 testing 때 noise 에 대한 marginalize 를 진행하게 된다.</p>

<h3 id="dropconnect">DropConnect</h3>

<p><em>dropout</em> 이 뉴런을 꺼버리는 것이었다면, <em>drop connect</em> 는 뉴런 간의 연결을 끝는 것이다. 즉 가중치(weight)=0 으로 하는 작업이 되겠다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image24.png" alt="drop connect" /></p>

<p>testing 때는 모든 연결이 활성화된다.</p>

<h3 id="fractional-max-pooling">Fractional Max Pooling</h3>

<p>이 방법은 pooling 을 응용한 것인데 일정한 pooling size 를 갖는 것이 아닌 <em>randomized pooling region</em> 을 fractional 하게 진행하는 것이다.<br />
굉장히 cool 한 방법이지만 많이 사용되지는 않는다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image25.png" alt="frational pooling" /></p>

<p>testing 때는 몇몇 영역에 대해 평균 예측값을 이용한다.</p>

<h3 id="stochastic-depth">Stochastic Depth</h3>

<p><em>stochastic depth</em> 는 depth 에 대해 한 번에 여러 번 내려갈 수 있도록 하는 방식이다.(skip some layers)</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image26.png" alt="stochastic detph" /></p>

<p>testing 때는 모든 레이어를 사용한다.</p>

<h3 id="cutout">Cutout</h3>

<p>이미지의 일부 영역을 가려버리는 방식이다. (일부 영역의 값을 0으로 처리)</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image27.png" alt="cutout" /></p>

<p>testing 때는 원본 이미지를 사용한다.</p>

<h3 id="mixup">Mixup</h3>

<p>두 개 이상의 이미지를 겹쳐서 학습시키는 방법이다. 이 때 target label 은 각각의 이미지가 합성된 비율이 된다.(그림에서는 cat:0.4, dog:0.6)</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image28.png" alt="cutout" /></p>

<p>testing 때는 원본 이미지를 사용한다.</p>

<hr />

<p>이처럼 <em>regularization</em> 은 학습 때와 평가할 때의 방법이 다르다는 것을 알고 가자.<br />
위에서 소개한 <em>regularization</em> 기법들은 각각 의미가 있지만 보통은 <strong><em>Batch Normalization</em></strong> 을 우선 적용하고 부족해 보이면 적용을 하게 된다.</p>

<p>몇 가지 팁을 더 주자면, 거대한 <em>FC layer</em> 에는 <em>dropout</em> 을 적용하는 것이 좋으며, <em>data augmentation</em> 은 배치 정규화와 마찬가지로 거의 모든 경우에 좋다.<br />
<em>cutout</em> 과 <em>mixup</em> 은 작은 분류 데이터셋(classification dataset)에 대해 적용해보면 좋다.</p>

<h2 id="choosing-hyperparameters">Choosing Hyperparameters</h2>

<p>우리는 최적화 알고리즘을 학습하면서 몇몇 설정해야하는 <strong>hyperparameter</strong> 를 보았다.<br />
이 <strong>hyperparameter</strong> 들은 굉장히 중요한 요소인데, 이를 어떻게 초기화하고 진행 과정에서 적절히 처리해주느냐에 따라 모델의 성능에 지대한 영향을 미친다.</p>

<p>그러나 우리는 보통 <strong>hyperparameter</strong> 의 값을 어떻게 설정해주어야하는지 정보가 없다.<br />
이는 많은 부분에 있어서 heuristic 에 의존하게 되는데, 조금이라도 체계적으로 접근해본다면 아래와 같다.</p>

<h3 id="step-1-initial-loss-확인">Step 1. initial loss 확인</h3>

<p>우리는 위에서 <em>weight decay</em> 개념을 학습했다. 그러나 처음에는 이것을 사용하지 않고 <strong>sanity check</strong> 를 통해 초기의 loss 가 이론적인 값과 일치하는지 확인해야한다.</p>

<p>예 : <em>Softmax</em> 에서 C개의 클래스를 분류하게 된다면 log(C) 가 나와야 함.</p>

<h3 id="step-2-작은-샘플에-대해-overfit">Step 2. 작은 샘플에 대해 overfit</h3>

<p>5~10 정도의 미니배치로 크기가 작은 데이터셋에 대해 100%의 train accuracy 가 나오게끔 학습을 시킨다. 이 부분에서 구조(architecture), <em>learning rate</em>, <em>weight</em> 의 값에 대한 여러 시도가 필요하다.</p>

<p>이러한 시도 중 loss 가 작아지지 않는다면 <em>learning rate</em> 가 너무 낮게 초기화 된 것이다.<br />
python 같은 언어를 사용할 때 NaN(또는 Inf 등) 의 수치는 loss explode 가 일어난 것이다. learning rate 가 너무 높게 초기화 되었으니 줄여야한다.</p>

<h3 id="step-3-loss-가-작아지도록-하는-learning-rate-찾기">Step 3. loss 가 작아지도록 하는 learning rate 찾기</h3>

<p>이전 스텝에서 찾은 구조를 이용해 모든 training data 에 대해 학습을 진행해본다. 이 때 <em>weight decay</em> 도 약간 사용하며, 100 회의 반복(iteration)적인 진행 과정 안에 loss 가 눈에 띄게 작아지는 learning rate 를 추려낸다.</p>

<p>실험적으로 learning rate는 1e-1, 1e-2, 1e-3, 1e-4 가 괜찮은 것으로 알려져있다.</p>

<h3 id="step-4-15-epoch-정도에서-coarse-grid엉성하게-몇-번-해보기">Step 4. 1~5 epoch 정도에서 Coarse grid(엉성하게 몇 번 해보기)</h3>

<p>위에서 찾은 몇몇 <em>learning rate</em> 와 <em>weight decay</em> 를 몇몇 모델에 대해 1~5 의 epoch 정도를 학습시킨다.</p>

<p>실험적으로 <em>weight decay</em> 는 1e-4, 1e-5, 0 이 괜찮은 것으로 알려져있다.</p>

<h3 id="step-5-grid-를-다듬고-조금-더-학습시키기시각화">Step 5. grid 를 다듬고 조금 더 학습시키기(시각화)</h3>

<p>이전 스텝에서 가장 괜찮은 모델에 대해 조금 더 길게(10~20 epoch)학습을 시켜본다. 이 때 l<em>earning rate decay</em> 는 사용하지 않는다.</p>

<h3 id="step-6-loss-의-변화를-확인">Step 6. loss 의 변화를 확인</h3>

<p>학습 과정을 시각화하여 진행이 어떻게 되어가는지 추적해야한다.<br />
이 과정에서 loss 를 좌표평면에 찍으면 noise 가 많이 발생할 것이기에 평균의 그래프를 그려서 시각화하는 것이 해석에 용이하다.</p>

<p>loss 가 감소하다가 어느 수준에서 평평한 그래프의 개형(plateau)을 띄게 된다면 <em>learning rate decay</em> 가 필요하다고 생각할 수 있다.<br />
그러나 loss 가 여전히 감소하고 있을 때 <em>learning rate decay</em> 를 적용한다면 멈춰버릴 수도 있다. 너무 빨리 decay 시키면 좋지 않다.</p>

<p>학습에는 오랜 시간이 걸릴 수 있기에 accuracy 가 올라가는 추세를 보이면 조금 더 학습할 수 있게 기다려야 한다.<br />
그러나 train / val 의 정확도 차이가 크거나 벌어지고 있다면 이는 <em>overfitting</em> 되고 있다는 뜻이다. 데이터를 더 모으고, <em>regularization</em> 을 적용해 모델이 조금 더 일반적일 수 있도록 하자.</p>

<p>train / val 모두 비슷한 정확도를 가지나 정확도 자체가 높지 않다면 이는 <em>underfitting</em> 된 것이다. 학습을 오래 시켜 보거나 bigger model 을 사용해보자.</p>

<h3 id="step-7-step-5로-돌아가서-반복">Step 7. step 5로 돌아가서 반복</h3>

<p>위와 같은 과정을 반복해 적절한 <em>hyperparameter</em> 들을 찾을 수 있다.<br />
이 때 하이퍼파라미터들을 처음 찾을 때 다음과 같이 두 가지 방법이 있다.</p>

<p><img src="/assets/images/2019-10-13---cs231n-traning-neural-networks-part-2/image29.png" alt="random and grid search" /></p>

<p>두 방법 중 좋은 탐색 방법은 무엇일까?<br />
<em>grid</em> 보다 <em>random</em> 이 더 좋은데, <em>grid</em> 를 왼쪽처럼 설정하게 된다면 최적의 공간을 탐색하지 못해버릴 수 있다.(그리드 위의 그래프의 고점 탐색 불가능)<br />
물론 그리드를 잘 짠다면 괜찮지만 현실적으로 그러기는 쉽지 않다는 사실을 생각하자.</p>

<p>추가적으로 weight update / weight magnitude 를 추적하는 것은 도움이 된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># assume parameter vector W and its gradient vector dW
</span><span class="n">param_scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">update</span> <span class="o">=</span> <span class="o">-</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">dW</span> <span class="c1"># simple SGD update
</span><span class="n">update_scale</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">update</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">W</span> <span class="o">+=</span> <span class="n">update</span> <span class="c1"># the actual update
</span><span class="k">print</span> <span class="n">update_scale</span> <span class="o">/</span> <span class="n">param_scale</span> <span class="c1"># want ~1e-3
</span></code></pre></div></div>

<p>업데이트와 값 사이의 비율을 추적하는 것인데 실험적으로 0.001(1e-3) 정도면 좋다고 한다.</p>

<hr />

<p>이번 강의를 정리하면 training error 를 잡기 위해서는 <em>optimizer</em> 와 <em>learning rate schedule</em> 이 필요하며,<br />
test error 를 잡기 위해서는 <em>regularization</em> 을 이용하고 <em>hyperparmeter</em> 를 적절히 선택함으로써 실현 가능하다.</p>

<blockquote>
  <p>이 포스트는 스탠포드의 <a href="http://cs231n.stanford.edu">cs231n</a> 8강 강의를 보고 공부 및 정리한 포스트입니다.<br />
잘못된 것이 있을 수 있습니다.<br />
댓글로 알려주시면 감사합니다!</p>
</blockquote>


<ul class="taxonomy__index">
  
  
    <li>
      <a href="#2020">
        <strong>2020</strong> <span class="taxonomy__count">11</span>
      </a>
    </li>
  
    <li>
      <a href="#2019">
        <strong>2019</strong> <span class="taxonomy__count">33</span>
      </a>
    </li>
  
</ul>



  <section id="2020" class="taxonomy__section">
    <h2 class="archive__subtitle">2020</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-04-01---postgresql-tips/" rel="permalink">PostgreSQL 간단 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">PostgreSQL 이란
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/docker/2020-03-27---docker-overview/" rel="permalink">Docker Overview
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Docker 란
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/docker/2020-03-27---docker-compose-overview/" rel="permalink">Docker Compose Overview
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Docker Compose
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-01-15---anaconda-tips/" rel="permalink">Anaconda Tips
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Anaconda 관련해서 자주 사용하고 필요한 것들을 정리해보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2020-01-13---pose-estimation-terms/" rel="permalink">Pose Estimation 관련 용어 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  2 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Pose Estimation 관련하여 자주 나오는 용어들에 대해 간략히 정리하고 틈틈히 상기할 수 있도록 해보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2020-01-13---boj-5373/" rel="permalink">[BOJ 5373] 큐빙
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">풀이
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2020-01-13---boj-17142/" rel="permalink">[BOJ 17142] 연구소3
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">문제 출처 : https://www.acmicpc.net/problem/17142
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2020-01-10---rmpe-retional-multi-person-pose-estimation/" rel="permalink">RMPE: Regional Multi-person Pose Estimation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">RMPE: Regional Multi-person Pose Estimation 라는 논문에 대해 간략히 학습하였고 이를 바탕으로 내용을 정리해보도록 하자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-01-10---linux-command-line/" rel="permalink">Linux Command Line 명령어 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">자주 쓰는 command 정리
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2020-01-09---vi-vim-command-line/" rel="permalink">VI VIM Command Line 명령어 정리
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">저장 및 종료
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2020-01-08---cascade-feature-aggregation-for-human-pose-estimation/" rel="permalink">Cascade Feature Aggregation for Human Pose Estimation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Cascade Feature Aggregation for Human Pose Estimation
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>

  <section id="2019" class="taxonomy__section">
    <h2 class="archive__subtitle">2019</h2>
    <div class="entries-list">
      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-12-14---graph-neural-networks-2/" rel="permalink">Graph Neural Networks 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Graph Pooling
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/computer%20network/2019-12-11---computer-network-overview-part-2/" rel="permalink">Computer Network Overview Part 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  24 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">지난 포스트에 이어서 컴퓨터 네트워크와 관련해 개략적으로 알아보자.
이번 포스트에서 다룰 것들은 다음과 같다.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-12-11---graph-neural-networks-1/" rel="permalink">Graph Neural Networks 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-12-14
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-09---digital-signature/" rel="permalink">Digital Signature
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">정보보호에서 Digital Signature 란 무엇인지 간략히 학습해보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-03---message-authentication-code/" rel="permalink">Message Authentication Code
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Message Authentication
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-12-02---cs231n-lec13-visualizing-and-understanding/" rel="permalink">[cs231n-lec13] Visualizing and Understanding
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">
  이 포스트는 스탠포드의 cs231n 13강 강의를 보고 공부 및 정리한 포스트입니다.
잘못된 것이 있을 수 있습니다.
댓글로 알려주시면 감사합니다!

</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-02---cryptographic-hash-functions/" rel="permalink">Cryptographic Hash Functions
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Hash Functions
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-02---other-public-key-cryptosystems/" rel="permalink">Other Public-Key Cryptosystems
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일: 2019-12-02
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-12-01---public-key-cryptography-and-rsa/" rel="permalink">Public-Key Cryptography and RSA
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  8 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일: 2019-12-01
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-11-25---introduction-to-number-theory-for-information-security-2/" rel="permalink">Introduction to Number Theory for Information Security 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">지난 포스트 에 이어 암호학에 있어서 어떤 수학적인 내용들이 사용되는지 살펴보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-11-22---cs231n-lec11-generative-models/" rel="permalink">[cs231n-lec11] Generative Models
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  18 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-11-29
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-11-19---cs231n-lec12-detection-and-segmentation/" rel="permalink">[cs231n-lec12] Detection and Segmentation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-11-22
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-11-01---introduction-to-number-theory-for-information-security-1/" rel="permalink">Introduction to Number Theory for Information Security 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  5 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">정보보호와 암호학에 대해 공부를 하다보면 마주하게 되는 수학이 있다.
여기서 정수론(Number Theory)에 대해 간략하게 맛보고 가보자.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-11-01---cs231n-lec10-recurrent-neural-networks/" rel="permalink">[cs231n-lec10] Recurrent Neural Networks
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  14 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-11-19
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2019-10-30---netlify-migration/" rel="permalink">Netlify 로 github 블로그 이전하기
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Netlify
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2019-10-18---boj-12886/" rel="permalink">[BOJ 12886] 돌 그룹
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">문제 출처 : https://www.acmicpc.net/problem/12886
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/computer%20network/2019-10-16---computer-network-overview/" rel="permalink">Computer Network Overview Part 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  30 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-22
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-15---block-cipher-operation/" rel="permalink">Block Cipher Operation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  11 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">이전 포스트 에서 DES 에서 더 보안성이 강화된 AES 까지 알아보았다.
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-14---cs231n-cnn-architectures/" rel="permalink">[cs231n-lec9] CNN Architectures
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  16 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-15
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-13---cs231n-traning-neural-networks-part-2/" rel="permalink">[cs231n-lec8] Training Neural Networks, Part 2
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  17 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-14
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-13---cs231n-training-neural-networks-part-1/" rel="permalink">[cs231n-lec7] Training Neural Networks, Part 1
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  10 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-13
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-12---cs231n-convolutional-neural-networks/" rel="permalink">[cs231n-lec5] Convolutional Neural Networks
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-12
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-12---advanced-encryption-standard/" rel="permalink">Advanced Encryption Standard
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">AES
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-11---cs231n-neural-networks-and-backpropagation/" rel="permalink">[cs231n-lec4] Neural Networks and Backpropagation
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-11
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-11---block-ciphers-and-the-data-encryption-standard/" rel="permalink">Block Ciphers and the Data Encryption Standard
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  6 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Block Ciphers
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-11---cs231n-loss-functions-and-optimization/" rel="permalink">[cs231n-lec3] Loss Functions and Optimization
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  9 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-17
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/machine%20learning/2019-10-10---cs231n-image-classification-pipeline/" rel="permalink">[cs231n-lec2] Image Classification Pipeline
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">최종 수정일 : 2019-10-16
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-10---classical-encryption-techniques/" rel="permalink">Classical Encryption Techniques
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  7 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">고전적인 암호화 기법
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/information%20security/2019-10-09---information-security-overview/" rel="permalink">정보보호(Information Security) 공부를 시작하며
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">정보보호란?
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2019-10-09---boj-2125/" rel="permalink">[BOJ 2125] Mothy
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  3 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">[BOJ 2125] Mothy
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2019-10-04---markdown-usage/" rel="permalink">Markdown을 써보자
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  4 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">Markdown 이란
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/boj/2019-10-02---boj-17503/" rel="permalink">[BOJ 17503] 맥주 축제
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  less than 1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">[BOJ 17503] 맥주 축제
</p>
  </article>
</div>

      
        



<div class="list__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/etc/2019-10-01---vs-code-c++-configuration-for-mac/" rel="permalink">맥에서 VS Code C++ 빌드 설정하기
</a>
      
    </h2>
    
      <p class="page__meta"><i class="far fa-clock" aria-hidden="true"></i> 




  1 minute read

</p>
    
    <p class="archive__item-excerpt" itemprop="description">맥에서 VS Code로 C++ 빌드를 해보자
</p>
  </article>
</div>

      
    </div>
    <a href="#page-title" class="back-to-top">Back to top &uarr;</a>
  </section>


  </div>
</div>
    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Cheong. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script src="https://kit.fontawesome.com/4eee35f757.js"></script>







    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/machine%20learning/2019-10-13---cs231n-traning-neural-networks-part-2/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/machine%20learning/--cs231n-traning-neural-networks-part-2"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://younghk-github-io.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
